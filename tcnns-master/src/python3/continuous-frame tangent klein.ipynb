{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'topnet' from '/home/ephy/Projects/tda_convolution/src/topnet.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2     # for capturing videos\n",
    "import math   # for mathematical operations\n",
    "import matplotlib.pyplot as plt    # for plotting the images\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "# from keras.preprocessing import image   # for preprocessing the images\n",
    "import numpy as np    # for mathematical operations\n",
    "# from keras.utils import np_utils\n",
    "# from skimage.transform import resize   # for resizing images\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random as rnd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "datadir = \"/home/ephy/Projects/tda_convolve_video/data/\"\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "device='cuda:0'\n",
    "\n",
    "from scipy.integrate import quad\n",
    "from scipy.integrate import dblquad\n",
    "from scipy.integrate import tplquad\n",
    "from scipy.ndimage import rotate as scipy_rotate\n",
    "from scipy.ndimage import shift as scipy_shift\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Import topnet utilities\n",
    "import sys\n",
    "sys.path.insert(0, '/home/ephy/Projects/tda_convolution/src/')\n",
    "import topnet\n",
    "import importlib\n",
    "importlib.reload(topnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(datadir+\"original/ucfTrainTestlist/trainlist01.txt\", \"r\")\n",
    "temp = f.read()\n",
    "videos = temp.split('\\n')\n",
    "\n",
    "# creating a dataframe having video names\n",
    "train = pd.DataFrame()\n",
    "train['video_name'] = videos\n",
    "train = train[:-1]\n",
    "\n",
    "# open the .txt file which have names of test videos\n",
    "f = open(datadir+\"original/ucfTrainTestlist/testlist01.txt\", \"r\")\n",
    "temp = f.read()\n",
    "videos = temp.split('\\n')\n",
    "\n",
    "# creating a dataframe having video names\n",
    "test = pd.DataFrame()\n",
    "test['video_name'] = videos\n",
    "test = test[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tags\n",
    "train_video_tag = []\n",
    "for i in range(train.shape[0]):\n",
    "    train_video_tag.append(train['video_name'][i].split('/')[0])\n",
    "    \n",
    "train['tag'] = train_video_tag\n",
    "\n",
    "test_video_tag = []\n",
    "for i in range(test.shape[0]):\n",
    "    test_video_tag.append(test['video_name'][i].split('/')[0])\n",
    "    \n",
    "test['tag'] = test_video_tag\n",
    "\n",
    "allVids = train.append(test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing at ~ 25fps we can get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13320/13320 [11:08<00:00, 19.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# storing the frames from training videos\n",
    "for i in tqdm(range(allVids.shape[0])):\n",
    "    count = 0\n",
    "    videoFile = allVids['video_name'][i]\n",
    "    cap = cv2.VideoCapture(datadir + 'original/UCF/'+videoFile.split(' ')[0].split('/')[1])   # capturing the video from the given path\n",
    "    frameRate = 1 #frame sample rate\n",
    "    x=1\n",
    "    i=0\n",
    "    while(cap.isOpened() and i<13):\n",
    "        frameId = cap.get(1) #current frame number\n",
    "        ret, frame = cap.read()\n",
    "        if (ret != True):\n",
    "            break\n",
    "        if (frameId % math.floor(frameRate) == 0):\n",
    "            # storing the frames in a new folder named train_1\n",
    "            filename =datadir+'altered/allUCF101_FFR/' + videoFile.split('/')[1].split(' ')[0] +\"_frame%d.jpg\" % count;count+=1\n",
    "            cv2.imwrite(filename, frame)\n",
    "            i+=1\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 173160/173160 [00:00<00:00, 449948.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# getting the names of all the images\n",
    "images = glob(datadir + \"altered/allUCF101_FFR/*.jpg\")\n",
    "\n",
    "all_image = []\n",
    "all_vid = []\n",
    "all_class = []\n",
    "all_frame = []\n",
    "for i in tqdm(range(len(images))):\n",
    "    n = images[i].split('/')[8]\n",
    "    all_image.append(n)\n",
    "    all_vid.append('_'.join(n.split('_')[1:4]))\n",
    "    all_class.append(n.split('_')[1])\n",
    "    all_frame.append(n.split('_')[4].split('.')[0].split('frame')[1])\n",
    "# # storing the images and their class in a dataframe\n",
    "all_data = pd.DataFrame()\n",
    "all_data['image'] = all_image\n",
    "all_data['class'] = all_class\n",
    "all_data['clip'] = all_vid\n",
    "all_data['frame'] = all_frame\n",
    "\n",
    "# # converting the dataframe into csv file \n",
    "all_data.to_csv(datadir + 'altered/all_data_guide_FFR.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 3-fold batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allVids = pd.read_csv(datadir + 'altered/all_data_guide_FFR.csv')\n",
    "allVids = allVids.sort_values(by=['clip', 'frame'])\n",
    "allVids_dedup = allVids.drop_duplicates(subset=['clip'], keep='first').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an interger based class to make tracking tests more simple\n",
    "classXwalk = pd.DataFrame({'class': np.unique(allVids_dedup['class'])})\n",
    "classXwalk['ind'] = np.array(list(range(0,101)))\n",
    "allVids_dedup['class_int'] = [int(classXwalk['ind'][classXwalk['class']==c]) for c in allVids_dedup['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Go through and load videos as 3d volume arrays\n",
    "# clips = np.unique(allVids['clip'])\n",
    "\n",
    "# all_image = []\n",
    "# for i in tqdm(range(len(clips))):\n",
    "#     stack = []\n",
    "#     for imageName in allVids[allVids['clip']==clips[i]]['image']:\n",
    "#         image = cv2.imread(datadir + 'altered/allUCF101_FFR/'+imageName)\n",
    "#         image = Image.fromarray(image , 'RGB')\n",
    "#         image = image.resize((100 , 100))\n",
    "#         image = np.array(image)/255\n",
    "#         image = np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "#         stack.append(image)\n",
    "#     all_image.append(np.array(stack))\n",
    "\n",
    "# all_image = np.array(all_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('/home/ephy/Projects/tda_convolve_video/data/altered/allVidStack100x100_FFR_13frames.npy',all_image)\n",
    "all_image = np.load('/home/ephy/Projects/tda_convolve_video/data/altered/allVidStack100x100_FFR_13frames.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=3, random_state=2, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=3, random_state=2)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load 3x5x5 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grids = topnet.legendre_klein_bottle(8,8,7,None)\n",
    "# np.save('/home/ephy/Projects/tda_convolve_video/src/python3/KleinFeatures_55.npy', grids)\n",
    "filters0 = np.load('/home/ephy/Projects/tda_convolve_video/src/python3//KleinFeatures_55.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1, 7, 7)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters0 = filters0.reshape(-1,1,7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters = np.load('/home/ephy/Projects/tda_convolve_video/src/python3/VideoFeatures_355.npy', allow_pickle=True)\n",
    "# With rotations\n",
    "filters = np.load('/home/ephy/Projects/tda_convolve_video/src/python3/VideoFeatures_355_movandrot.npy', allow_pickle=True)\n",
    "flat_filters = filters.reshape(-1, *filters.shape[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv0 = nn.Conv3d(1, 64, (1,7,7),stride=(1,2,2))\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(64, 180, (3,5,5))\n",
    "        self.conv2 = nn.Conv3d(180, 180, (3,5,5))\n",
    "        self.conv3 = nn.Conv3d(180, 180, (3,3,3))\n",
    "        self.conv4 = nn.Conv3d(180, 36, (1,3,3))\n",
    "        \n",
    "        x= torch.randn(1,1,9,112,112)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 750)\n",
    "        self.fc2 = nn.Linear(750, 200)\n",
    "        self.fc3 = nn.Linear(200, 101)\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm1d(self._to_linear)\n",
    "        self.batch2 = nn.BatchNorm1d(750)\n",
    "        self.batch3 = nn.BatchNorm1d(200)\n",
    "#         self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        \n",
    "    def convs(self, x):\n",
    "        x = F.max_pool3d(F.relu(self.conv0(x)),(1,3,3),stride=(1,2,2))\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]*x[0].shape[3]\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # reshape inside of array\n",
    "        x = self.convs(x)\n",
    "#         x = self.drop1(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "#         x = self.batch1(x)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "#         x = self.batch2(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.batch3(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08227020800000001"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "torch.cuda.memory_allocated()*1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, X, y, batchSize):\n",
    "    confusionMatrix = np.zeros([101,101], dtype=np.int8)\n",
    "    testingSeq = list(range(0,y.shape[0]+1,batchSize))\n",
    "    testingSeq.append(y.shape[0]+1)\n",
    "    testingSeq = np.array(testingSeq)\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(testingSeq)-1):\n",
    "            data = X[testingSeq[i]:testingSeq[i+1]].view(-1,1,9,100,100).to(device)\n",
    "            out = torch.argmax(net(data),axis=1).cpu()\n",
    "            yt = torch.argmax(y[testingSeq[i]:testingSeq[i+1]],axis=1)\n",
    "            for pred,label in zip(out,yt):\n",
    "                confusionMatrix[label][pred] +=1\n",
    "        del data\n",
    "        del out        \n",
    "        return confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Reset.\n",
      "Fold:  0 Epoch:  0\n",
      "Accuracy:  0.0905368516833485\n",
      "Fold:  0 Epoch:  1\n",
      "Accuracy:  0.15764331210191082\n",
      "Fold:  0 Epoch:  2\n",
      "Accuracy:  0.24522292993630573\n",
      "Fold:  0 Epoch:  3\n",
      "Accuracy:  0.33371246587807096\n",
      "Fold:  0 Epoch:  4\n",
      "Accuracy:  0.3862602365787079\n",
      "Fold:  0 Epoch:  5\n",
      "Accuracy:  0.4442675159235669\n",
      "Fold:  0 Epoch:  6\n",
      "Accuracy:  0.4752047315741583\n",
      "Fold:  0 Epoch:  7\n",
      "Accuracy:  0.4986351228389445\n",
      "Fold:  0 Epoch:  8\n",
      "Accuracy:  0.5161510464058234\n",
      "Fold:  0 Epoch:  9\n",
      "Accuracy:  0.5238853503184714\n",
      "Fold:  0 Epoch:  10\n",
      "Accuracy:  0.531164695177434\n",
      "Fold:  0 Epoch:  11\n",
      "Accuracy:  0.5302547770700637\n",
      "Fold:  0 Epoch:  12\n",
      "Accuracy:  0.5400363967242948\n",
      "Fold:  0 Epoch:  13\n",
      "Accuracy:  0.5450409463148317\n",
      "Fold:  0 Epoch:  14\n",
      "Accuracy:  0.5402638762511374\n",
      "Fold:  0 Epoch:  15\n",
      "Accuracy:  0.54049135577798\n",
      "Fold:  0 Epoch:  16\n",
      "Accuracy:  0.5475432211101001\n",
      "Fold:  0 Epoch:  17\n",
      "Accuracy:  0.543221110100091\n",
      "Fold:  0 Epoch:  18\n",
      "Accuracy:  0.551410373066424\n",
      "Fold:  0 Epoch:  19\n",
      "Accuracy:  0.543221110100091\n",
      "Fold:  0 Epoch:  20\n",
      "Accuracy:  0.5534576888080073\n",
      "Fold:  0 Epoch:  21\n",
      "Accuracy:  0.551410373066424\n",
      "Fold:  0 Epoch:  22\n",
      "Accuracy:  0.552547770700637\n",
      "Fold:  0 Epoch:  23\n",
      "Accuracy:  0.5461783439490446\n",
      "Fold:  0 Epoch:  24\n",
      "Accuracy:  0.5454959053685169\n",
      "Fold:  0 Epoch:  25\n",
      "Accuracy:  0.5484531392174704\n",
      "Fold:  0 Epoch:  26\n",
      "Accuracy:  0.5464058234758872\n",
      "Fold:  0 Epoch:  27\n",
      "Accuracy:  0.5429936305732485\n",
      "Fold:  0 Epoch:  28\n",
      "Accuracy:  0.5425386715195633\n",
      "Fold:  0 Epoch:  29\n",
      "Accuracy:  0.5491355777979982\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-59ad05668462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingSeq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "# Batch size of 26+1, because normalization requires same batch size and 4472%26=0 (very close to even split) (same logic for training)\n",
    "testingBatchSize=20\n",
    "trainingBatchSize=10\n",
    "results = [[],[],[]]\n",
    "lossrec = [[],[],[]]\n",
    "\n",
    "rnd.seed(1)\n",
    "classMat = np.eye(101,dtype=np.float)\n",
    "for foldIndex in range(3):\n",
    "# for foldIndex, (train_index, test_index) in enumerate(skf.split(all_image, allVids_dedup['class'])):\n",
    "    net = Net().to(device)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "    # Set filters in 2 layers\n",
    "    with torch.no_grad():\n",
    "        for i,weights in enumerate(flat_filters):\n",
    "            net.conv1.weight[i][0] = torch.nn.Parameter(torch.tensor(weights)).to(device)\n",
    "#             for g,filts in enumerate(net.conv2.weight):\n",
    "#                 net.conv2.weight[i][g] = torch.nn.Parameter(torch.tensor(flat_filters[g])).to(device)\n",
    "        net.conv1.requires_grad=False\n",
    "#         net.conv2.requires_grad=False\n",
    "    print('Network Reset.')\n",
    "\n",
    "#     ## Check our work\n",
    "#     with torch.no_grad():\n",
    "#         for i,weights in enumerate(flat_filters):\n",
    "#             print(net.conv2.weight.shape)\n",
    "#             plt.imshow(net.conv2.weight[i][9][0].cpu(), cmap='gray')\n",
    "#             plt.show\n",
    "#             break        \n",
    "                \n",
    "    #Shuffle does not actually work, so just manually shuffle...\n",
    "#     rnd.shuffle(train_index)\n",
    "#     rnd.shuffle(test_index)\n",
    "#     y_train = allVids_dedup.iloc[train_index]['class_int']\n",
    "#     y_test = allVids_dedup.iloc[test_index]['class_int']\n",
    "#     X_train = all_image[train_index]\n",
    "#     X_test = all_image[test_index]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.stack(all_image), allVids_dedup['class_int'],\n",
    "                                                        shuffle=True, random_state=foldIndex,\n",
    "                                                        test_size=0.33, stratify = allVids_dedup['class_int'])\n",
    "\n",
    "\n",
    "    y_train = torch.tensor([classMat[c] for c in y_train],dtype=torch.float32)\n",
    "    y_test = torch.tensor([classMat[c] for c in y_test],dtype=torch.float32)\n",
    "    X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "    X_test =  torch.tensor(X_test,dtype=torch.float32)\n",
    "    y_train = torch.tensor(np.array(y_train),dtype=torch.float32)\n",
    "    y_test = torch.tensor(np.array(y_test),dtype=torch.float32)\n",
    "\n",
    "    trainingSeq = list(range(0,y_train.shape[0],trainingBatchSize))\n",
    "    trainingSeq.append(y_train.shape[0]+1)\n",
    "    trainingSeq = np.array(trainingSeq)\n",
    "\n",
    "    # Grab 5 tests in final epoch (4 + last one run after training loop)\n",
    "    finalEpochTests = list(range(int((len(trainingSeq)-1)/5),len(trainingSeq)-1,int((len(trainingSeq)-1)/5)))\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in range(len(trainingSeq)-1):\n",
    "            X=X_train[trainingSeq[i]:trainingSeq[i+1]].view(-1,1,7,100,100)\n",
    "            X=X.to(device)\n",
    "            y=y_train[trainingSeq[i]:trainingSeq[i+1]]\n",
    "            y=y.to(device)\n",
    "            net.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = nn.CrossEntropyLoss()(output, torch.argmax(y, axis=1))\n",
    "            lossrec[foldIndex].append(float(loss.tolist()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del loss\n",
    "            del output\n",
    "            del X\n",
    "            del y\n",
    "            if epoch==(EPOCHS-1) and (i in finalEpochTests):\n",
    "                print(foldIndex,'Final Epoch',i)\n",
    "                results[foldIndex].append(test(net, X_test, y_test, testingBatchSize))\n",
    "        print('Fold: ', foldIndex, 'Epoch: ',epoch)\n",
    "        results[foldIndex].append(test(net, X_test, y_test, testingBatchSize))\n",
    "        cm = results[foldIndex][len(results[foldIndex])-1]\n",
    "        print('Accuracy: ' , np.sum(np.diag(cm))/np.sum(cm))\n",
    "\n",
    "    np.save('/home/ephy/Projects/tda_convolve_video/data/models/results/losses_1kflayer355_4layer_fold'+str(foldIndex)+'.npy',lossrec)\n",
    "    np.save('/home/ephy/Projects/tda_convolve_video/data/models/results/confuseMats_1kflayer355_4layer_fold'+str(foldIndex)+'.npy',results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13320, 9, 100, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test opening first layer mid-train (works)\n",
    "# Test more frames\n",
    "all_image=all_image[:,0:9,:,:]\n",
    "all_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0 Epoch:  0\n",
      "Accuracy:  0.5657415832575068\n",
      "Fold:  0 Epoch:  1\n",
      "Accuracy:  0.5666515013648772\n",
      "Fold:  0 Epoch:  2\n",
      "Accuracy:  0.564604185623294\n",
      "Fold:  0 Epoch:  3\n",
      "Accuracy:  0.5648316651501365\n",
      "Fold:  0 Epoch:  4\n",
      "Accuracy:  0.565059144676979\n",
      "Fold:  0 Epoch:  5\n",
      "Accuracy:  0.5675614194722475\n",
      "Fold:  0 Epoch:  6\n",
      "Accuracy:  0.5686988171064604\n",
      "Fold:  0 Epoch:  7\n",
      "Accuracy:  0.5625568698817106\n",
      "Fold:  0 Epoch:  8\n",
      "Accuracy:  0.5630118289353958\n",
      "Fold:  0 Epoch:  9\n",
      "Accuracy:  0.5609645131938126\n",
      "Fold:  0 Epoch:  10\n",
      "Accuracy:  0.5614194722474978\n",
      "Fold:  0 Epoch:  11\n",
      "Accuracy:  0.5727934485896269\n",
      "Fold:  0 Epoch:  12\n",
      "Accuracy:  0.5621019108280255\n",
      "Fold:  0 Epoch:  13\n",
      "Accuracy:  0.5673339399454049\n",
      "Fold:  0 Epoch:  14\n",
      "Accuracy:  0.5659690627843494\n",
      "Fold:  0 Epoch:  15\n",
      "Accuracy:  0.5605095541401274\n",
      "Fold:  0 Epoch:  16\n",
      "Accuracy:  0.5550500454959054\n",
      "Fold:  0 Epoch:  17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-366ada54677b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestingBatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fold: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoldIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Epoch: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestingBatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: '\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-16f84d602f21>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net, X, y, batchSize)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "# Batch size of 26+1, because normalization requires same batch size and 4472%26=0 (very close to even split) (same logic for training)\n",
    "testingBatchSize=20\n",
    "trainingBatchSize=10\n",
    "results = [[],[],[]]\n",
    "lossrec = [[],[],[]]\n",
    "\n",
    "rnd.seed(1)\n",
    "classMat = np.eye(101,dtype=np.float)\n",
    "for foldIndex in range(3):\n",
    "# for foldIndex, (train_index, test_index) in enumerate(skf.split(all_image, allVids_dedup['class'])):\n",
    "    net = Net().to(device)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "    # Set filters in 2 layers\n",
    "    with torch.no_grad():\n",
    "        for i,weights in enumerate(flat_filters):\n",
    "            net.conv1.weight[i][0] = torch.nn.Parameter(torch.tensor(weights)).to(device)\n",
    "#             for g,filts in enumerate(net.conv2.weight):\n",
    "#                 net.conv2.weight[i][g] = torch.nn.Parameter(torch.tensor(flat_filters[g])).to(device)\n",
    "        net.conv1.requires_grad=False\n",
    "#         net.conv2.requires_grad=False\n",
    "    print('Network Reset.')\n",
    "\n",
    "#     ## Check our work\n",
    "#     with torch.no_grad():\n",
    "#         for i,weights in enumerate(flat_filters):\n",
    "#             print(net.conv2.weight.shape)\n",
    "#             plt.imshow(net.conv2.weight[i][9][0].cpu(), cmap='gray')\n",
    "#             plt.show\n",
    "#             break        \n",
    "                \n",
    "    #Shuffle does not actually work, so just manually shuffle...\n",
    "#     rnd.shuffle(train_index)\n",
    "#     rnd.shuffle(test_index)\n",
    "#     y_train = allVids_dedup.iloc[train_index]['class_int']\n",
    "#     y_test = allVids_dedup.iloc[test_index]['class_int']\n",
    "#     X_train = all_image[train_index]\n",
    "#     X_test = all_image[test_index]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.stack(all_image), allVids_dedup['class_int'],\n",
    "                                                        shuffle=True, random_state=foldIndex,\n",
    "                                                        test_size=0.33, stratify = allVids_dedup['class_int'])\n",
    "\n",
    "\n",
    "    y_train = torch.tensor([classMat[c] for c in y_train],dtype=torch.float32)\n",
    "    y_test = torch.tensor([classMat[c] for c in y_test],dtype=torch.float32)\n",
    "    X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "    X_test =  torch.tensor(X_test,dtype=torch.float32)\n",
    "    y_train = torch.tensor(np.array(y_train),dtype=torch.float32)\n",
    "    y_test = torch.tensor(np.array(y_test),dtype=torch.float32)\n",
    "\n",
    "    trainingSeq = list(range(0,y_train.shape[0],trainingBatchSize))\n",
    "    trainingSeq.append(y_train.shape[0]+1)\n",
    "    trainingSeq = np.array(trainingSeq)\n",
    "\n",
    "    # Grab 5 tests in final epoch (4 + last one run after training loop)\n",
    "    finalEpochTests = list(range(int((len(trainingSeq)-1)/5),len(trainingSeq)-1,int((len(trainingSeq)-1)/5)))\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in range(len(trainingSeq)-1):\n",
    "            X=X_train[trainingSeq[i]:trainingSeq[i+1]].view(-1,1,9,100,100)\n",
    "            X=X.to(device)\n",
    "            y=y_train[trainingSeq[i]:trainingSeq[i+1]]\n",
    "            y=y.to(device)\n",
    "            net.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = nn.CrossEntropyLoss()(output, torch.argmax(y, axis=1))\n",
    "            lossrec[foldIndex].append(float(loss.tolist()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del loss\n",
    "            del output\n",
    "            del X\n",
    "            del y\n",
    "            if epoch==(EPOCHS-1) and (i in finalEpochTests):\n",
    "                print(foldIndex,'Final Epoch',i)\n",
    "                results[foldIndex].append(test(net, X_test, y_test, testingBatchSize))\n",
    "        print('Fold: ', foldIndex, 'Epoch: ',epoch)\n",
    "        results[foldIndex].append(test(net, X_test, y_test, testingBatchSize))\n",
    "        cm = results[foldIndex][len(results[foldIndex])-1]\n",
    "        print('Accuracy: ' , np.sum(np.diag(cm))/np.sum(cm))\n",
    "\n",
    "    np.save('/home/ephy/Projects/tda_convolve_video/data/models/results/losses_1kflayer355_4layer_fold'+str(foldIndex)+'.npy',lossrec)\n",
    "    np.save('/home/ephy/Projects/tda_convolve_video/data/models/results/confuseMats_1kflayer355_4layer_fold'+str(foldIndex)+'.npy',results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0 Epoch:  0\n",
      "Accuracy:  0.17333939945404914\n",
      "Fold:  0 Epoch:  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c43b4fea9690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestingBatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fold: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoldIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Epoch: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestingBatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: '\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-92ff29aa9f1f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net, X, y, batchSize)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "# Batch size of 26+1, because normalization requires same batch size and 4472%26=0 (very close to even split) (same logic for training)\n",
    "testingBatchSize=20\n",
    "trainingBatchSize=20\n",
    "results = [[],[],[]]\n",
    "lossrec = [[],[],[]]\n",
    "\n",
    "rnd.seed(1)\n",
    "classMat = np.eye(101,dtype=np.float)\n",
    "for foldIndex in range(3):\n",
    "# for foldIndex, (train_index, test_index) in enumerate(skf.split(all_image, allVids_dedup['class'])):\n",
    "    net = Net().to(device)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "    # Set filters in 2 layers\n",
    "    with torch.no_grad():\n",
    "        for i,weights in enumerate(flat_filters):\n",
    "            net.conv1.weight[i][0] = torch.nn.Parameter(torch.tensor(weights)).to(device)\n",
    "#             for g,filts in enumerate(net.conv2.weight):\n",
    "#                 net.conv2.weight[i][g] = torch.nn.Parameter(torch.tensor(flat_filters[g])).to(device)\n",
    "        net.conv1.requires_grad=False\n",
    "#         net.conv2.requires_grad=False\n",
    "    print('Network Reset.')\n",
    "\n",
    "#     ## Check our work\n",
    "#     with torch.no_grad():\n",
    "#         for i,weights in enumerate(flat_filters):\n",
    "#             print(net.conv2.weight.shape)\n",
    "#             plt.imshow(net.conv2.weight[i][9][0].cpu(), cmap='gray')\n",
    "#             plt.show\n",
    "#             break        \n",
    "                \n",
    "    #Shuffle does not actually work, so just manually shuffle...\n",
    "#     rnd.shuffle(train_index)\n",
    "#     rnd.shuffle(test_index)\n",
    "#     y_train = allVids_dedup.iloc[train_index]['class_int']\n",
    "#     y_test = allVids_dedup.iloc[test_index]['class_int']\n",
    "#     X_train = all_image[train_index]\n",
    "#     X_test = all_image[test_index]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.stack(all_image), allVids_dedup['class_int'],\n",
    "                                                        shuffle=True, random_state=foldIndex,\n",
    "                                                        test_size=0.33, stratify = allVids_dedup['class_int'])\n",
    "\n",
    "\n",
    "    y_train = torch.tensor([classMat[c] for c in y_train],dtype=torch.float32)\n",
    "    y_test = torch.tensor([classMat[c] for c in y_test],dtype=torch.float32)\n",
    "    X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "    X_test =  torch.tensor(X_test,dtype=torch.float32)\n",
    "    y_train = torch.tensor(np.array(y_train),dtype=torch.float32)\n",
    "    y_test = torch.tensor(np.array(y_test),dtype=torch.float32)\n",
    "\n",
    "    trainingSeq = list(range(0,y_train.shape[0],trainingBatchSize))\n",
    "    trainingSeq.append(y_train.shape[0]+1)\n",
    "    trainingSeq = np.array(trainingSeq)\n",
    "\n",
    "    # Grab 5 tests in final epoch (4 + last one run after training loop)\n",
    "    finalEpochTests = list(range(int((len(trainingSeq)-1)/5),len(trainingSeq)-1,int((len(trainingSeq)-1)/5)))\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in range(len(trainingSeq)-1):\n",
    "            X=X_train[trainingSeq[i]:trainingSeq[i+1]].view(-1,1,7,100,100)\n",
    "            X=X.to(device)\n",
    "            y=y_train[trainingSeq[i]:trainingSeq[i+1]]\n",
    "            y=y.to(device)\n",
    "            net.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = nn.CrossEntropyLoss()(output, torch.argmax(y, axis=1))\n",
    "            lossrec[foldIndex].append(float(loss.tolist()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del loss\n",
    "            del output\n",
    "            del X\n",
    "            del y\n",
    "            if epoch==(EPOCHS-1) and (i in finalEpochTests):\n",
    "                print(foldIndex,'Final Epoch',i)\n",
    "                results[foldIndex].append(test(net, X_test, y_test, testingBatchSize))\n",
    "        print('Fold: ', foldIndex, 'Epoch: ',epoch)\n",
    "        results[foldIndex].append(test(net, X_test, y_test, testingBatchSize))\n",
    "        cm = results[foldIndex][len(results[foldIndex])-1]\n",
    "        print('Accuracy: ' , np.sum(np.diag(cm))/np.sum(cm))\n",
    "\n",
    "    np.save('/home/ephy/Projects/tda_convolve_video/data/models/results/losses_1kflayer355_4layer_fold'+str(foldIndex)+'.npy',lossrec)\n",
    "    np.save('/home/ephy/Projects/tda_convolve_video/data/models/results/confuseMats_1kflayer355_4layer_fold'+str(foldIndex)+'.npy',results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.09440400363967243\n"
     ]
    }
   ],
   "source": [
    "results[foldIndex].append(test(net, X_test, y_test, testingBatchSize))\n",
    "cm = results[foldIndex][len(results[foldIndex])-1]\n",
    "print('Accuracy: ' , np.sum(np.diag(cm))/np.sum(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convRescale = nn.Conv3d(1, 64, (1,7,7))\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv3d(64, 180, (3,5,5))\n",
    "        self.conv2 = nn.Conv3d(180, 180, (3,5,5))\n",
    "        self.conv3 = nn.Conv3d(180, 180, (3,5,5))\n",
    "        self.conv4 = nn.Conv3d(180, 180, (3,5,5))\n",
    "        self.conv5 = nn.Conv3d(180, 36, (1,5,5))\n",
    "        \n",
    "        self.batchConv1 = nn.BatchNorm3d(180)\n",
    "        self.batchConv2 = nn.BatchNorm3d(180)\n",
    "        self.batchConv3 = nn.BatchNorm3d(36)\n",
    "        \n",
    "        self.down1 = nn.Upsample((5,92,92),mode='trilinear',align_corners=False)\n",
    "        self.down2 = nn.Upsample((1,84,84),mode='trilinear',align_corners=False)\n",
    "\n",
    "        x= torch.randn(1,1,9,100,100)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 700)\n",
    "        self.fc2 = nn.Linear(700, 200)\n",
    "        self.fc3 = nn.Linear(200, 101)        \n",
    "        \n",
    "        self.batch1 = nn.BatchNorm1d(self._to_linear)\n",
    "        self.batch2 = nn.BatchNorm1d(750)\n",
    "        self.batch3 = nn.BatchNorm1d(200)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        \n",
    "    def convs(self, x):\n",
    "        x = F.max_pool3d(self.convRescale(x),(1,3,3))\n",
    "        print(x.shape)\n",
    "        inp1 = self.down1(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batchConv1(x)\n",
    "        inp2 = self.down2(x)\n",
    "        x+=inp1\n",
    "        del inp1\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.batchConv2(x)\n",
    "        x+=inp2\n",
    "        del inp2\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.batchConv3(x)\n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]*x[0].shape[3]\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.batch3(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 9, 31, 31])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (23) must match the size of tensor b (92) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e5b0d4fbaebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_allocated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-e90b238c45c7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_linear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-e90b238c45c7>\u001b[0m in \u001b[0;36mconvs\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchConv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0minp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0minp1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minp1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (23) must match the size of tensor b (92) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "net = ResNet().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "torch.cuda.memory_allocated()*1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image = all_image[:,0:9,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Reset.\n",
      "Fold:  0 Epoch:  0\n",
      "Accuracy:  0.13034576888080074\n",
      "Fold:  0 Epoch:  1\n",
      "Accuracy:  0.23589626933575977\n",
      "Fold:  0 Epoch:  2\n",
      "Accuracy:  0.3237033666969973\n",
      "Fold:  0 Epoch:  3\n",
      "Accuracy:  0.38967242948134667\n",
      "Fold:  0 Epoch:  4\n",
      "Accuracy:  0.4356232939035487\n",
      "Fold:  0 Epoch:  5\n",
      "Accuracy:  0.48316651501364877\n",
      "Fold:  0 Epoch:  6\n",
      "Accuracy:  0.5095541401273885\n",
      "Fold:  0 Epoch:  7\n",
      "Accuracy:  0.5336669699727025\n",
      "Fold:  0 Epoch:  8\n",
      "Accuracy:  0.5452684258416742\n",
      "Fold:  0 Epoch:  9\n",
      "Accuracy:  0.5534576888080073\n",
      "Fold:  0 Epoch:  10\n",
      "Accuracy:  0.5464058234758872\n",
      "Fold:  0 Epoch:  11\n",
      "Accuracy:  0.5570973612374887\n",
      "Fold:  0 Epoch:  12\n",
      "Accuracy:  0.564604185623294\n",
      "Fold:  0 Epoch:  13\n",
      "Accuracy:  0.5648316651501365\n",
      "Fold:  0 Epoch:  14\n",
      "Accuracy:  0.5625568698817106\n",
      "Fold:  0 Epoch:  15\n",
      "Accuracy:  0.5666515013648772\n",
      "Fold:  0 Epoch:  16\n",
      "Accuracy:  0.5639217470427661\n",
      "Fold:  0 Epoch:  17\n",
      "Accuracy:  0.5609645131938126\n",
      "Fold:  0 Epoch:  18\n",
      "Accuracy:  0.5555050045495905\n",
      "Fold:  0 Epoch:  19\n",
      "Accuracy:  0.5466333030027297\n",
      "Fold:  0 Epoch:  20\n",
      "Accuracy:  0.552547770700637\n",
      "Fold:  0 Epoch:  21\n",
      "Accuracy:  0.552547770700637\n",
      "Fold:  0 Epoch:  22\n",
      "Accuracy:  0.5520928116469518\n",
      "Fold:  0 Epoch:  23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-241f5827e4f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestingBatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fold: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoldIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Epoch: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestingBatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: '\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-92ff29aa9f1f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net, X, y, batchSize)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtestingSeq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "# Batch size of 26+1, because normalization requires same batch size and 4472%26=0 (very close to even split) (same logic for training)\n",
    "testingBatchSize=10\n",
    "trainingBatchSize=10\n",
    "results = [[],[],[]]\n",
    "lossrec = [[],[],[]]\n",
    "foldIndex=0\n",
    "rnd.seed(1)\n",
    "classMat = np.eye(101,dtype=np.float)\n",
    "# for foldIndex in range(3):\n",
    "# for foldIndex, (train_index, test_index) in enumerate(skf.split(all_image, allVids_dedup['class'])):\n",
    "net = ResNet().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "# Set filters in 2 layers\n",
    "with torch.no_grad():\n",
    "    for i,weights in enumerate(flat_filters):\n",
    "        net.conv1.weight[i][0] = torch.nn.Parameter(torch.tensor(weights)).to(device)\n",
    "#             for g,filts in enumerate(net.conv2.weight):\n",
    "#                 net.conv2.weight[i][g] = torch.nn.Parameter(torch.tensor(flat_filters[g])).to(device)\n",
    "    net.conv1.requires_grad=False\n",
    "#         net.conv2.requires_grad=False\n",
    "print('Network Reset.')\n",
    "\n",
    "#     ## Check our work\n",
    "#     with torch.no_grad():\n",
    "#         for i,weights in enumerate(flat_filters):\n",
    "#             print(net.conv2.weight.shape)\n",
    "#             plt.imshow(net.conv2.weight[i][9][0].cpu(), cmap='gray')\n",
    "#             plt.show\n",
    "#             break        \n",
    "\n",
    "#Shuffle does not actually work, so just manually shuffle...\n",
    "#     rnd.shuffle(train_index)\n",
    "#     rnd.shuffle(test_index)\n",
    "#     y_train = allVids_dedup.iloc[train_index]['class_int']\n",
    "#     y_test = allVids_dedup.iloc[test_index]['class_int']\n",
    "#     X_train = all_image[train_index]\n",
    "#     X_test = all_image[test_index]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.stack(all_image), allVids_dedup['class_int'],\n",
    "                                                    shuffle=True, random_state=foldIndex,\n",
    "                                                    test_size=0.33, stratify = allVids_dedup['class_int'])\n",
    "\n",
    "\n",
    "y_train = torch.tensor([classMat[c] for c in y_train],dtype=torch.float32)\n",
    "y_test = torch.tensor([classMat[c] for c in y_test],dtype=torch.float32)\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "X_test =  torch.tensor(X_test,dtype=torch.float32)\n",
    "y_train = torch.tensor(np.array(y_train),dtype=torch.float32)\n",
    "y_test = torch.tensor(np.array(y_test),dtype=torch.float32)\n",
    "\n",
    "trainingSeq = list(range(0,y_train.shape[0],trainingBatchSize))\n",
    "trainingSeq.append(y_train.shape[0]+1)\n",
    "trainingSeq = np.array(trainingSeq)\n",
    "\n",
    "# Grab 5 tests in final epoch (4 + last one run after training loop)\n",
    "finalEpochTests = list(range(int((len(trainingSeq)-1)/5),len(trainingSeq)-1,int((len(trainingSeq)-1)/5)))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(len(trainingSeq)-1):\n",
    "        X=X_train[trainingSeq[i]:trainingSeq[i+1]].view(-1,1,9,100,100)\n",
    "        X=X.to(device)\n",
    "        y=y_train[trainingSeq[i]:trainingSeq[i+1]]\n",
    "        y=y.to(device)\n",
    "        net.zero_grad()\n",
    "        output = net(X)\n",
    "        loss = nn.CrossEntropyLoss()(output, torch.argmax(y, axis=1))\n",
    "        lossrec[foldIndex].append(float(loss.tolist()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del loss\n",
    "        del output\n",
    "        del X\n",
    "        del y\n",
    "        if epoch==(EPOCHS-1) and (i in finalEpochTests):\n",
    "            print(foldIndex,'Final Epoch',i)\n",
    "            results[foldIndex].append(test(net, X_test, y_test, testingBatchSize))\n",
    "    print('Fold: ', foldIndex, 'Epoch: ',epoch)\n",
    "    results[foldIndex].append(test(net, X_test, y_test, testingBatchSize))\n",
    "    cm = results[foldIndex][len(results[foldIndex])-1]\n",
    "    print('Accuracy: ' , np.sum(np.diag(cm))/np.sum(cm))\n",
    "\n",
    "np.save('/home/ephy/Projects/tda_convolve_video/data/models/results/losses_1kflayer355_4layer_fold'+str(foldIndex)+'.npy',lossrec)\n",
    "np.save('/home/ephy/Projects/tda_convolve_video/data/models/results/confuseMats_1kflayer355_4layer_fold'+str(foldIndex)+'.npy',results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD BATCHNORM TO ALL LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convRescale = nn.Conv3d(1, 180, (1,7,7))\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv3d(180, 180, (3,5,5))\n",
    "        self.conv2 = nn.Conv3d(180, 180, (3,5,5))\n",
    "        self.conv3 = nn.Conv3d(180, 180, (3,5,5))\n",
    "        self.conv4 = nn.Conv3d(180, 180, (3,5,5))\n",
    "        self.conv5 = nn.Conv3d(180, 75, (3,5,5))\n",
    "        \n",
    "        self.batchConv1 = nn.BatchNorm3d(180)\n",
    "        self.batchConv2 = nn.BatchNorm3d(180)\n",
    "        self.batchConv3 = nn.BatchNorm3d(75)\n",
    "        \n",
    "        self.up = nn.Upsample((9,31,31),mode='trilinear',align_corners=False)\n",
    "        \n",
    "        x= torch.randn(1,1,9,100,100)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 700)\n",
    "        self.fc2 = nn.Linear(700, 200)\n",
    "        self.fc3 = nn.Linear(200, 101)        \n",
    "        \n",
    "        self.batch1 = nn.BatchNorm1d(self._to_linear)\n",
    "        self.batch2 = nn.BatchNorm1d(750)\n",
    "        self.batch3 = nn.BatchNorm1d(200)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        \n",
    "    def convs(self, x):\n",
    "        x = F.max_pool3d(self.convRescale(x),(1,3,3))\n",
    "        inp1 = x\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batchConv1(x)\n",
    "        x = self.up(x)\n",
    "        inp2 = x\n",
    "        x+=inp1\n",
    "        del inp1\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.batchConv2(x)\n",
    "        x = self.up(x)\n",
    "        x+=inp2\n",
    "        del inp2\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.batchConv3(x)\n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]*x[0].shape[3]\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.batch3(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1214182400000001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "torch.cuda.memory_allocated()*1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
