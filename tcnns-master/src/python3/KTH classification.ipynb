{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2     # for capturing videos\n",
    "import math   # for mathematical operations\n",
    "import matplotlib.pyplot as plt    # for plotting the images\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "# from keras.preprocessing import image   # for preprocessing the images\n",
    "import numpy as np    # for mathematical operations\n",
    "# from keras.utils import np_utils\n",
    "# from skimage.transform import resize   # for resizing images\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random as rnd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "datadir = \"/home/ephy/Projects/tda_convolve_video/data/original/KTH/\"\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "device='cuda:0'\n",
    "\n",
    "from scipy.integrate import quad\n",
    "from scipy.integrate import dblquad\n",
    "from scipy.integrate import tplquad\n",
    "from scipy.ndimage import rotate as scipy_rotate\n",
    "from scipy.ndimage import shift as scipy_shift\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(datadir)\n",
    "classes = [c for c in classes if c!='scenes.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(datadir+'scenes.txt', \"r\")\n",
    "scenes = []\n",
    "for x in f:\n",
    "    if '-' in x:\n",
    "        parts = x.split('\\t')\n",
    "        name = parts[0].strip()+'_uncomp.avi'\n",
    "        parts = parts[-1] \n",
    "        parts = parts.split(', ')\n",
    "        starts = [part.split('-')[0] for part in parts]\n",
    "        starts = np.array(starts, dtype=np.int)\n",
    "        scenes.append((name,starts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "videos = []\n",
    "y = []\n",
    "for c in classes:\n",
    "    d = os.listdir(datadir + c)\n",
    "    for f in d:\n",
    "        y.append(c)\n",
    "        videos.append(f)\n",
    "labels=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.repeat(labels, [len(scene[1]) for scene in scenes])\n",
    "ds = [scene[0].split('_')[2] for scene in scenes for s in scene[1]]\n",
    "len(ds)\n",
    "persons = [scene[0].split('_')[0] for scene in scenes for s in scene[1]]\n",
    "# nums = [str(i) for scene in scenes for i in range(len(scene[1]))]\n",
    "stratLabels = [lab+d+person for (lab,d,person) in zip(labels,ds,persons)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(stratLabels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Factor of 2.77 shrink getws us very close to 100^2 pixels, same as experiments before, but with aspect preserved\n",
    "# imageSize=np.array([np.round(160/1.3), np.round(120/1.3)],dtype=np.int)\n",
    "imageSize=np.array([90,60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [(videos[i],y[i]) for i in range(len(y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # storing the frames from training videos\n",
    "# all_image = []\n",
    "# for (video,clss) in tqdm(videos):\n",
    "#     count = 0\n",
    "#     cap = cv2.VideoCapture(datadir+clss+'/'+video)   # capturing the video from the given path\n",
    "#     frameRate = 1 #frame sample rate\n",
    "#     frames = []\n",
    "#     startPoints = list(filter(lambda x: x[0]==video, scenes))\n",
    "#     startPoints = startPoints[0][1]-1 # (Indexed from 1)\n",
    "#     while(cap.isOpened()):\n",
    "#         frameId = cap.get(1) #current frame number\n",
    "#         ret, frame = cap.read()\n",
    "#         if (ret != True):\n",
    "#             break\n",
    "#         if (frameId % math.floor(frameRate) == 0):\n",
    "#             frames.append(frame)\n",
    "#             count+=1\n",
    "#     cap.release()\n",
    "#     frames = np.stack(frames)\n",
    "#     for start in startPoints:\n",
    "#         clip = []\n",
    "#         for f in range(start,start+23):\n",
    "#             image = frames[f]\n",
    "#             image = Image.fromarray(image , 'RGB')\n",
    "#             image = image.crop((35,30,160-35,120-30)) #(left, top, right, bottom)\n",
    "#             image = np.array(image)/255\n",
    "#             image = np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "#             clip.append(image)\n",
    "#         all_image.append(clip)\n",
    "# all_image = np.stack(all_image)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # storing the frames from training videos\n",
    "# all_image = []\n",
    "# for (video,clss) in tqdm(videos):\n",
    "#     count = 0\n",
    "#     cap = cv2.VideoCapture(datadir+clss+'/'+video)   # capturing the video from the given path\n",
    "#     frameRate = 1 #frame sample rate\n",
    "#     frames = []\n",
    "#     startPoints = list(filter(lambda x: x[0]==video, scenes))\n",
    "#     startPoints = startPoints[0][1]-1 # (Indexed from 1)\n",
    "#     while(cap.isOpened()):\n",
    "#         frameId = cap.get(1) #current frame number\n",
    "#         ret, frame = cap.read()\n",
    "#         if (ret != True):\n",
    "#             break\n",
    "#         if (frameId % math.floor(frameRate) == 0):\n",
    "#             frames.append(frame)\n",
    "#             count+=1\n",
    "#     cap.release()\n",
    "#     frames = np.stack(frames)\n",
    "#     for start in startPoints:\n",
    "#         clip = []\n",
    "#         for f in range(start,start+23):\n",
    "#             image = frames[f]\n",
    "#             image = Image.fromarray(image , 'RGB')\n",
    "#             image = image.resize(imageSize)\n",
    "#             image = np.array(image)/255\n",
    "#             image = np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "#             clip.append(image)\n",
    "#         all_image.append(clip)\n",
    "# all_image = np.stack(all_image)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('/home/ephy/Projects/tda_convolve_video/data/altered/KTH_FFR_23frames_focal.npy',all_image)\n",
    "# all_image=np.load('/home/ephy/Projects/tda_convolve_video/data/altered/KTH_FFR_23frames_focal.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('/home/ephy/Projects/tda_convolve_video/data/altered/KTH_FFR_23frames.npy',all_image)\n",
    "all_image=np.load('/home/ephy/Projects/tda_convolve_video/data/altered/KTH_FFR_23frames.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageSize=(23,90,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters77_r = np.load('/home/ephy/Projects/tda_convolve_video/src/python3/VideoFeatures_5x11x11_movAndRot120.npy', allow_pickle=True)\n",
    "filters77 = np.load('/home/ephy/Projects/tda_convolve_video/src/python3/VideoFeatures_5x11x11_movAndRot120.npy', allow_pickle=True)\n",
    "filters_2 = np.load('/home/ephy/Projects/tda_convolve_video/src/python3/VideoFeatures_394_movAndRot320.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, X, y, batchSize):\n",
    "    confusionMatrix = np.zeros([101,101], dtype=np.int8)\n",
    "    testingSeq = list(range(0,y.shape[0],batchSize))\n",
    "    testingSeq.append(y.shape[0]+1)\n",
    "    testingSeq = np.array(testingSeq)\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(testingSeq)-1):\n",
    "            data = X[testingSeq[i]:testingSeq[i+1]].view(-1,1,*imageSize).to(device)\n",
    "            out = torch.argmax(net(data),axis=1).cpu()\n",
    "            yt = torch.argmax(y[testingSeq[i]:testingSeq[i+1]],axis=1)\n",
    "            for pred,label in zip(out,yt):\n",
    "                confusionMatrix[label][pred] +=1\n",
    "    del data\n",
    "    del out\n",
    "    del yt\n",
    "    return confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convRescale = nn.Conv3d(1, 120, (5,11,11), bias=False,stride=(1,3,3))\n",
    "        self.conv1 = nn.Conv3d(120, 120, (5,11,11), bias=False,stride=(1,2,2))\n",
    "        self.conv2 = nn.Conv3d(120, 320, (3,9,4),bias=False)\n",
    "        \n",
    "        self.batchConv1 = nn.BatchNorm3d(120)\n",
    "        self.batchConv2 = nn.BatchNorm3d(120)\n",
    "        self.batchConv3 = nn.BatchNorm3d(320)\n",
    "        \n",
    "        self.drop1=nn.Dropout3d(.2)\n",
    "        self.drop2=nn.Dropout3d(.2)\n",
    "                \n",
    "        x= torch.randn(1,1,*imageSize)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 6)\n",
    "        self.batch1 = nn.BatchNorm1d(6)\n",
    "        \n",
    "    def convs(self, x):\n",
    "        x = F.leaky_relu(self.convRescale(x))\n",
    "#         x = self.batchConv1(x)\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "#         x = self.batchConv2(x)\n",
    "#         x = self.drop1(x)\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "#         x = self.batchConv3(x)\n",
    "#         x = self.drop2(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]*x[0].shape[3]\n",
    "#             print(self._to_linear)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch1(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052834304000000006"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "torch.cuda.memory_allocated()*1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(pd.get_dummies(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Reset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ephy/anaconda3/envs/opencvenv/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/ephy/anaconda3/envs/opencvenv/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0 Epoch:  0\n",
      "Accuracy:  0.3458333333333333\n",
      "Fold:  0 Epoch:  1\n",
      "Accuracy:  0.4125\n",
      "Fold:  0 Epoch:  2\n",
      "Accuracy:  0.5708333333333333\n",
      "Fold:  0 Epoch:  3\n",
      "Accuracy:  0.6541666666666667\n",
      "Fold:  0 Epoch:  4\n",
      "Accuracy:  0.7083333333333334\n",
      "Fold:  0 Epoch:  5\n",
      "Accuracy:  0.7458333333333333\n",
      "Fold:  0 Epoch:  6\n",
      "Accuracy:  0.7833333333333333\n",
      "Fold:  0 Epoch:  7\n",
      "Accuracy:  0.8125\n",
      "Fold:  0 Epoch:  8\n",
      "Accuracy:  0.8125\n",
      "Fold:  0 Epoch:  9\n",
      "Accuracy:  0.8208333333333333\n",
      "Fold:  0 Epoch:  10\n",
      "Accuracy:  0.8125\n",
      "Fold:  0 Epoch:  11\n",
      "Accuracy:  0.7833333333333333\n",
      "Fold:  0 Epoch:  12\n",
      "Accuracy:  0.825\n",
      "Fold:  0 Epoch:  13\n",
      "Accuracy:  0.8208333333333333\n",
      "Fold:  0 Epoch:  14\n",
      "Accuracy:  0.8458333333333333\n",
      "Fold:  0 Epoch:  15\n",
      "Accuracy:  0.8375\n",
      "Fold:  0 Epoch:  16\n",
      "Accuracy:  0.8208333333333333\n",
      "Fold:  0 Epoch:  17\n",
      "Accuracy:  0.8166666666666667\n",
      "Fold:  0 Epoch:  18\n",
      "Accuracy:  0.8125\n",
      "Fold:  0 Epoch:  19\n",
      "Accuracy:  0.825\n",
      "Fold:  0 Epoch:  20\n",
      "Accuracy:  0.8375\n",
      "Fold:  0 Epoch:  21\n",
      "Accuracy:  0.8333333333333334\n",
      "Fold:  0 Epoch:  22\n",
      "Accuracy:  0.8458333333333333\n",
      "Fold:  0 Epoch:  23\n",
      "Accuracy:  0.85\n",
      "Fold:  0 Epoch:  24\n",
      "Accuracy:  0.8458333333333333\n",
      "Fold:  0 Epoch:  25\n",
      "Accuracy:  0.8375\n",
      "Fold:  0 Epoch:  26\n",
      "Accuracy:  0.8416666666666667\n",
      "Fold:  0 Epoch:  27\n",
      "Accuracy:  0.85\n",
      "Fold:  0 Epoch:  28\n",
      "Accuracy:  0.8083333333333333\n",
      "Fold:  0 Epoch:  29\n",
      "Accuracy:  0.8416666666666667\n",
      "Fold:  0 Epoch:  30\n",
      "Accuracy:  0.8666666666666667\n",
      "Fold:  0 Epoch:  31\n",
      "Accuracy:  0.85\n",
      "Fold:  0 Epoch:  32\n",
      "Accuracy:  0.8541666666666666\n",
      "Fold:  0 Epoch:  33\n",
      "Accuracy:  0.8708333333333333\n",
      "Fold:  0 Epoch:  34\n",
      "Accuracy:  0.8166666666666667\n",
      "Fold:  0 Epoch:  35\n",
      "Accuracy:  0.8041666666666667\n",
      "Fold:  0 Epoch:  36\n",
      "Accuracy:  0.8375\n",
      "Fold:  0 Epoch:  37\n",
      "Accuracy:  0.8333333333333334\n",
      "Fold:  0 Epoch:  38\n",
      "Accuracy:  0.8458333333333333\n",
      "Fold:  0 Epoch:  39\n",
      "Accuracy:  0.8625\n",
      "Fold:  0 Epoch:  40\n",
      "Accuracy:  0.8416666666666667\n",
      "Fold:  0 Epoch:  41\n",
      "Accuracy:  0.8416666666666667\n",
      "Fold:  0 Epoch:  42\n",
      "Accuracy:  0.8458333333333333\n",
      "Fold:  0 Epoch:  43\n",
      "Accuracy:  0.8375\n",
      "Fold:  0 Epoch:  44\n",
      "Accuracy:  0.8333333333333334\n",
      "Fold:  0 Epoch:  45\n",
      "Accuracy:  0.8416666666666667\n",
      "Fold:  0 Epoch:  46\n",
      "Accuracy:  0.8416666666666667\n",
      "Fold:  0 Epoch:  47\n",
      "Accuracy:  0.8375\n",
      "Fold:  0 Epoch:  48\n",
      "Accuracy:  0.8416666666666667\n",
      "Fold:  0 Epoch:  49\n",
      "Accuracy:  0.8583333333333333\n",
      "Fold:  0 Epoch:  50\n",
      "Accuracy:  0.8583333333333333\n",
      "Fold:  0 Epoch:  51\n",
      "Accuracy:  0.8583333333333333\n",
      "Fold:  0 Epoch:  52\n",
      "Accuracy:  0.85\n",
      "Fold:  0 Epoch:  53\n",
      "Accuracy:  0.8416666666666667\n",
      "Fold:  0 Epoch:  54\n",
      "Accuracy:  0.8458333333333333\n",
      "Fold:  0 Epoch:  55\n",
      "Accuracy:  0.8583333333333333\n",
      "Fold:  0 Epoch:  56\n",
      "Accuracy:  0.85\n",
      "Fold:  0 Epoch:  57\n",
      "Accuracy:  0.8583333333333333\n",
      "Fold:  0 Epoch:  58\n",
      "Accuracy:  0.8541666666666666\n",
      "Fold:  0 Epoch:  59\n",
      "Accuracy:  0.8458333333333333\n",
      "Fold:  0 Epoch:  60\n",
      "Accuracy:  0.8541666666666666\n",
      "Fold:  0 Epoch:  61\n",
      "Accuracy:  0.8583333333333333\n",
      "Fold:  0 Epoch:  62\n",
      "Accuracy:  0.85\n",
      "Fold:  0 Epoch:  63\n",
      "Accuracy:  0.8541666666666666\n",
      "Fold:  0 Epoch:  64\n",
      "Accuracy:  0.85\n",
      "Fold:  0 Epoch:  65\n",
      "Accuracy:  0.8458333333333333\n",
      "Fold:  0 Epoch:  66\n",
      "Accuracy:  0.8625\n",
      "Fold:  0 Epoch:  67\n",
      "Accuracy:  0.85\n",
      "Fold:  0 Epoch:  68\n",
      "Accuracy:  0.8416666666666667\n",
      "Fold:  0 Epoch:  69\n",
      "Accuracy:  0.8291666666666667\n",
      "Fold:  0 Epoch:  70\n",
      "Accuracy:  0.825\n",
      "Fold:  0 Epoch:  71\n",
      "Accuracy:  0.8458333333333333\n",
      "Fold:  0 Epoch:  72\n",
      "Accuracy:  0.8208333333333333\n",
      "Fold:  0 Epoch:  73\n",
      "Accuracy:  0.85\n",
      "Fold:  0 Epoch:  74\n",
      "Accuracy:  0.8541666666666666\n",
      "Fold:  0 Epoch:  75\n",
      "Accuracy:  0.8458333333333333\n",
      "Fold:  0 Epoch:  76\n",
      "Accuracy:  0.8416666666666667\n",
      "Fold:  0 Epoch:  77\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  78\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  79\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  80\n",
      "Accuracy:  0.8666666666666667\n",
      "Fold:  0 Epoch:  81\n",
      "Accuracy:  0.8708333333333333\n",
      "Fold:  0 Epoch:  82\n",
      "Accuracy:  0.8458333333333333\n",
      "Fold:  0 Epoch:  83\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  84\n",
      "Accuracy:  0.8833333333333333\n",
      "Fold:  0 Epoch:  85\n",
      "Accuracy:  0.8833333333333333\n",
      "Fold:  0 Epoch:  86\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  87\n",
      "Accuracy:  0.8708333333333333\n",
      "Fold:  0 Epoch:  88\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  89\n",
      "Accuracy:  0.8708333333333333\n",
      "Fold:  0 Epoch:  90\n",
      "Accuracy:  0.8666666666666667\n",
      "Fold:  0 Epoch:  91\n",
      "Accuracy:  0.8583333333333333\n",
      "Fold:  0 Epoch:  92\n",
      "Accuracy:  0.8625\n",
      "Fold:  0 Epoch:  93\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  94\n",
      "Accuracy:  0.8666666666666667\n",
      "Fold:  0 Epoch:  95\n",
      "Accuracy:  0.8666666666666667\n",
      "Fold:  0 Epoch:  96\n",
      "Accuracy:  0.8625\n",
      "Fold:  0 Epoch:  97\n",
      "Accuracy:  0.8625\n",
      "Fold:  0 Epoch:  98\n",
      "Accuracy:  0.8625\n",
      "Fold:  0 Epoch:  99\n",
      "Accuracy:  0.8708333333333333\n",
      "Fold:  0 Epoch:  100\n",
      "Accuracy:  0.8625\n",
      "Fold:  0 Epoch:  101\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  102\n",
      "Accuracy:  0.8625\n",
      "Fold:  0 Epoch:  103\n",
      "Accuracy:  0.8583333333333333\n",
      "Fold:  0 Epoch:  104\n",
      "Accuracy:  0.8666666666666667\n",
      "Fold:  0 Epoch:  105\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  106\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  107\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  108\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  109\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  110\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  111\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  112\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  113\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  114\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  115\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  116\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  117\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  118\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  119\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  120\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  121\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  122\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  123\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  124\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  125\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  126\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  127\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  128\n",
      "Accuracy:  0.8791666666666667\n",
      "Fold:  0 Epoch:  129\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  130\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  131\n",
      "Accuracy:  0.875\n",
      "Fold:  0 Epoch:  132\n",
      "Accuracy:  0.8666666666666667\n",
      "Fold:  0 Epoch:  133\n",
      "Accuracy:  0.8333333333333334\n",
      "Fold:  0 Epoch:  134\n",
      "Accuracy:  0.8\n",
      "Fold:  0 Epoch:  135\n",
      "Accuracy:  0.8333333333333334\n",
      "Fold:  0 Epoch:  136\n",
      "Accuracy:  0.8166666666666667\n",
      "Fold:  0 Epoch:  137\n",
      "Accuracy:  0.8375\n",
      "Fold:  0 Epoch:  138\n",
      "Accuracy:  0.8375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b7b6560666ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mlossrec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfoldIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "# Batch size of 26+1, because normalization requires same batch size and 4472%26=0 (very close to even split) (same logic for training)\n",
    "testingBatchSize=100\n",
    "trainingBatchSize=100\n",
    "results = [[],[],[]]\n",
    "lossrec = [[],[],[]]\n",
    "\n",
    "for foldIndex in range(1):\n",
    "\n",
    "    net = Net().to(device)\n",
    "    optimizer = optim.AdamW(net.parameters(), lr=9e-3)\n",
    "\n",
    "    # Set filters in 2 layers\n",
    "    with torch.no_grad():\n",
    "        for i,weights in enumerate(filters77_r):\n",
    "            net.convRescale.weight[i] = torch.nn.Parameter(torch.tensor(weights)).to(device)\n",
    "        for i,weights in enumerate(filters77_r):\n",
    "            net.conv1.weight[:,i,...] = torch.nn.Parameter(torch.tensor(filters77)).to(device)\n",
    "        for i,weights in enumerate(filters77):\n",
    "            net.conv2.weight[:,i,...] = torch.nn.Parameter(torch.tensor(filters_2)).to(device)\n",
    "        net.convRescale.requires_grad=False\n",
    "        net.conv1.requires_grad=False\n",
    "        net.conv2.requires_grad=False\n",
    "\n",
    "    print('Network Reset.')  \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_image, labels,\n",
    "                                                        shuffle=True, random_state=foldIndex,\n",
    "                                                        test_size=0.1, stratify = stratLabels)\n",
    "\n",
    "    y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "    X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "    X_test =  torch.tensor(X_test,dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "\n",
    "    trainingSeq = list(range(0,y_train.shape[0],trainingBatchSize))\n",
    "    trainingSeq.append(y_train.shape[0]+1)\n",
    "    trainingSeq = np.array(trainingSeq)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in range(len(trainingSeq)-1):\n",
    "            X=X_train[trainingSeq[i]:trainingSeq[i+1]].view(-1,1,*imageSize)\n",
    "            X=X.to(device)\n",
    "            y=y_train[trainingSeq[i]:trainingSeq[i+1]]\n",
    "            y=y.to(device)\n",
    "            net.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = nn.CrossEntropyLoss()(output, torch.argmax(y, axis=1))\n",
    "            lossrec[foldIndex].append(float(loss.tolist()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del loss\n",
    "            del output\n",
    "            del X\n",
    "            del y\n",
    "            \n",
    "        print('Fold: ', foldIndex, 'Epoch: ',epoch)\n",
    "        results[foldIndex].append(test(net, X_test, y_test, testingBatchSize))\n",
    "        cm = results[foldIndex][len(results[foldIndex])-1]\n",
    "        print('Accuracy: ' , np.sum(np.diag(cm))/np.sum(cm))\n",
    "\n",
    "# np.save('/home/ephy/Projects/tda_convolve_video/data/altered/KTH_net8_k1layer_30_res.npy', results)\n",
    "# np.save('/home/ephy/Projects/tda_convolve_video/data/altered/KTH_net8_k1layer_30_loss.npy', lossrec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f891e2598d0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXhU1fkH8O+bjbCENWEVDGsQUBYjoCKCKCJY96po61KVYq3+LC5FcUet1lpb3CgiUtTiilUBEaoiICCEPUDYg4QtCQSyQPbz+2PuJDOT2edO7pLv53l8mLlz5973xJl3zj33LKKUAhERWV+M0QEQEZE+mNCJiGyCCZ2IyCaY0ImIbIIJnYjIJuKMOnFycrJKTU016vRERJa0bt26fKVUirfXDEvoqampyMjIMOr0RESWJCL7fb3GJhciIptgQicisgkmdCIim2BCJyKyCSZ0IiKbYEInIrIJJnQiIpuwXELfcaQIry7egfziMqNDISIyFcsl9N25xXj9+904XlJudChERKYSMKGLyCwRyRWRTB+vtxCRr0Vkk4hsFZE79Q+zVow4/q2q5sIcRESugqmhzwYwxs/r9wHYppTqD2AEgFdFJCHy0LyL0TJ6NVdaIiJyEzChK6WWATjubxcASSIiAJpp+1bqE15dMaIl9OponYGIyJr0aEN/A8BZAA4B2ALg/5RSXtOtiEwQkQwRycjLywvrZLFaxFWsoRMRudEjoV8OYCOAjgAGAHhDRJp721EpNUMpla6USk9J8Tr7Y0AibHIhIvJGj4R+J4B5ymE3gH0AeutwXK9ia5pcmNCJiFzpkdB/ATAKAESkHYA0AHt1OK5XsTU3RaN1BiIiawq4wIWIzIWj90qyiOQAeBpAPAAopaYDmApgtohsASAA/qyUyo9WwMJui0REXgVM6Eqp8QFePwRgtG4RBeBsclFsQycicmO5kaLOfujs5UJE5M56CV3Yhk5E5I0FE7rjX/ZyISJyZ7mEHsuh/0REXlkuoTubXNjLhYjInWUTOvM5EZE76yV0LWI2uRARubNcQo9lkwsRkVeWS+icD52IyDvrJXSthv7iwu0GR0JEZC6WS+jOJpejhVwkmojIleUSOof8ExF5Z72E7nIz9NCJ0wZGQkRkLpZL6K7+9PFGo0MgIjINCyb02hr6nrwSA+MgIjIXyyX0uJjakPOLeWOUiMjJcgk9Nbmp0SEQEZlSwIQuIrNEJFdEMn28/oiIbNT+yxSRKhFprX+oRETkTzA19NkAxvh6USn1ilJqgFJqAIDHAPyolDquU3xERBSkgAldKbUMQLAJejyAuRFFREREYdGtDV1EmsBRk//czz4TRCRDRDLy8vL0OjUREUHfm6K/AvCTv+YWpdQMpVS6Uio9JSVFx1MTEZGeCf1msLmFiMgwuiR0EWkB4GIAX+pxPCIiCl1coB1EZC6AEQCSRSQHwNMA4gFAKTVd2+1aAIuVUhy6SURkkIAJXSk1Poh9ZsPRvZGIiAxiuZGinhSn0yUiAmCDhP5pRo7RIRARmYLlE/rGnBNGh0BEZAqWT+j/+fkXo0MgIjIFyyd0IiJyYEInIrIJJnQiIpuwZEKPixGjQyAiMh1LJvQz2zQxOgQiItOxZEL/zz1D3Z4XlVYYFAkRkXlYMqG3a57o9vzhTzcZFAkRkXlYMqEDQNOE2JrHB0+cNjASIiJzsGxC5wwuRETuLJvQT1dU1Tzm/FxERBZO6EziRETuLJvQL+jepuaxsFs6EVHghC4is0QkV0Qy/ewzQkQ2ishWEflR3xC9u/PCrvVxGiIiywimhj4bwBhfL4pISwBvAbhKKdUXwK/1CS14bH4hIgoioSullgE47meXWwDMU0r9ou2fq1NsQTtaWFrfpyQiMh092tB7AWglIktFZJ2I3KbDMQNyXXquoopVdCKigItEB3mMcwGMAtAYwCoRWa2U2um5o4hMADABALp06RLRSV1TeEVVdUTHIiKyAz1q6DkAFimlSpRS+QCWAejvbUel1AylVLpSKj0lJUWHUztUsoZORKRLQv8SwEUiEiciTQAMAbBdh+P65XojtLyqGm8v3RPtUxIRmVow3RbnAlgFIE1EckTkLhGZKCITAUAptR3AIgCbAawBMFMp5bOLo17S2ie5PX95UVa0T0lEZGoB29CVUuOD2OcVAK/oElGQuiY3rc/TERGZnmVHihIRkTsmdCKT+mxdDnKLOMaCgseETmRCuUWlePjTTbjn3xlGh0IWwoROZELOrri5RWUGR0JWwoRORGQTTOhERDbBhE5EZBOWTuhTr+nn9nzboUKDIiEiMp6lE3pcjPtSRWOnLTcoEqLo4Fz/FApLJ/RuHC1KNsVlFSkclk7oQ7q1CbwTkQWxZk7hsHRCJ7I71tQpFEzoREQ2wYRORGQTTOhEJsa2dAoFEzqRCbHtnMLBhE5EZBPBLEE3S0RyRcTrsnIiMkJETorIRu2/p/QPk6hhYVMLhSPgEnQAZgN4A8AcP/ssV0pdqUtERFSDTS8UioA1dKXUMgDH6yEWIiKKgF5t6OeLyCYR+UZE+vraSUQmiEiGiGTk5eXpdGoi+2LTC4VCj4S+HsCZSqn+AF4H8F9fOyqlZiil0pVS6SkpKTqcGph6tc/fDyLLYlNLdL2/KhupkxegtKLK6FB0FXFCV0oVKqWKtccLAcSLSHLEkQXpsj7t3Z5zCl0iCmTa97sBAIWnKwyORF8RJ3QRaS/iqE+IyGDtmMciPW6wYjxKwCl0iaihCtjLRUTmAhgBIFlEcgA8DSAeAJRS0wHcAOBeEakEcBrAzUrVX8tfDK9NiYgABJHQlVLjA7z+BhzdGg3RrFEwPS+JrIU3Qykclh8pmhgfW2dbWaW9bnQQEQXD8gndm7QnFhkdAlFE2JJI4bBlQgeAn/fW231ZIiJTsG1Cv2nGaqNDICKqV7ZN6EREDQ0TOhGRTTChE5mYAvsvUvCY0ImIbIIJncjEBOy/SMGzdUI/WliKG6evwrHiMqNDISKKOlsn9CEvfoc12cfx0doDRodCRBR1tk7oREQNSYNI6BxGTVbFXi4UigaR0EvKKpE6eQG+3HjQ6FCIgsKboRSOBpHQcwpOAwDe+mGPwZEQEUVPg0joW7Vl6Xj5SlbBzyqFo0Ek9N25xUaHQBSW+mx62ZxzAp+ty6m385H+AiZ0EZklIrkikhlgv/NEpEpEbtAvvPrDRTGoobvqjZ/w8KebjA6DIhBMDX02gDH+dhCRWAAvA/hWh5hCds9FXYPaz9eyXosyDyPtiUXYeuikjlERRY5NLxSKgAldKbUMwPEAu90P4HMAuXoEFaqRvdtG9P7vsxxhb8lhQidzYC8XCkfEbegi0gnAtQCmB7HvBBHJEJGMvLy8SE+tG+eXh3UhIrIyPW6K/gPAn5VSARuhlVIzlFLpSqn0lJQUHU4dGl8J2znwiCutE5GVxelwjHQAH4kjKyYDGCsilUqp/+pw7KiqrKpGcVklR5ISkS1EnNCVUjV3JEVkNoD5Zk3myqMKPuWLTHyccQA3nHuG43U2upBJ8LNI4QiY0EVkLoARAJJFJAfA0wDiAUApFbDd3My+2nQIgKOmDrDJhYisLWBCV0qND/ZgSqk7IoomyvbkleCdZXtxz/BuAGprQcI2FzIZ9nKhcDSIkaKuXli43egQiIiiosEldG+cdSG2uBCRlTXIhF6htZnXqMnoTOlEZF22SOhNEkLrrHPltBUoraiqyd9sryQiO7BFQh/QuSWmXt036P13HC1C7ycXoazSUVOvGVgUjeCIImDERePhk6fr/6SkC1skdAD4dXrnsN/LFheiWh+s3m90CBQm2yT0SNQO/WdGJ3MxokctmyCtyzYJPZIP/icZOdox+EEmIuuyTULXA2voRMAbP+w2OgQKExO6ixe/ycLJ0xU1z7cdKuSiF0QmU12tUF3Nypc3tknocTGRF6W8shovL8qqeT522nKMm7Yi4uMShYsXjXUN/ct3GPT8koiOYde/q20SemyM4K83nBPxccorqwPvRBRlvJ3jW25RGU6cqgi8YwNkm4QOAO2aJxodAhFZgF1/MG2V0If3TI74GFVsmyMTsGuTAEWXrRK6Ht0Ov9hwUIdIiPRh15qkHp6fv40VMA+2Suh6yTpSGPVz3DxjFabO3xb18xDZ1cwV+5CRfdzoMEwlYEIXkVkikisimT5ev1pENovIRhHJEJFh+odZv8b8Y3mdbeWV1dh5tEi3c6zeexzvrtin2/HIntj04h//PO6CqaHPBjDGz+vfAeivlBoA4HcAZuoQl+lMnb8No19bhkMnOHERRR+bWigcARO6UmoZAJ/XNUqpYlU7xLIpDP7RTGuXFJXjrtUu7QpOlUfl+ERUf+x65aNLG7qIXCsiWQAWwFFL97XfBK1ZJiMvL0+PU9fxycTz8b9JwyM+Dtu3ichqdEnoSqkvlFK9AVwDYKqf/WYopdKVUukpKSl6nLqOFo3j0aNtEto1bxTRcXy1b3PgEZH12bVJK7SlfgJQSi0Tke4ikqyUytfz2Ebal1+CrCOOG6LXvrUSCXExuKxPOzz9qz5om8TBTKQ/uzYJUHRFXEMXkR6idQAXkUEAEgAci/S4kWrWSL/fqvEzVrs9L6+sxoLNhzFhzjrdzkFEoeMPn7tgui3OBbAKQJqI5IjIXSIyUUQmartcDyBTRDYCeBPATcoE89D26dhCt2OVlFd63X68hDdIKTrs2iSgt9eW7ESl56LvDVjAaqxSanyA118G8LJuEZmQr+9Wtcvv1p68Yhw9WYoLeiRjxrI96NSyCcad06F+AiTS2c6jRegVpR5jelqTfRyvLN6Buy7siracy8m+I0WvG9hJt2MVlnqvoSsFlFZUobyyGqNe/RG3zPwZAPDiwizc95/1up2fqL499aXXcYSm9K8f92L4Kz8YHYYp6HpT1ExG9m4b9XMopdD7yUVRPw/ZT3FZJbYePIkh3doYHYpX1RZrxSitsFjAUWLbGrqr5onR+d0y/EYBWdYf/7MeN81YjQKT3oeptFpGJwANJKFvfubyqBzX263fRZlHonIuspfthx0TwJUFGNdgVKWhirUVS7J1Qr+4V3QGLzlVe8non2YciOo5yR6M7wfmXxVr6JZk2zZ0AJh5e3pUR3bmFpXV2bb1UPSn3iX7c/asMirxsyegNdm6hh4fG4OmOg4wCsaRwtKax679Y7cfLsS5U5dg2Mvf12s8ROFgDd2abJ3QjeacD2ZR5hFc8c/lOFZSjpwC9+l3R726FAeOnzIiPDKQs+IdeACRMVV0LgRkTUzoUXT8VDle+TYLEz/wPUXAnrwSzFmVXW8xkbUY1eRSzYxuSQ0moU//zSBcP+iM+j2pAt78YU/g3fjdIR+M+mh4u+FP5tdgEvqYfh1w/yU96vWc/1q2N6j9+NVpuMw6ZUsVE7olNZiEDgCpyU2NDgEAsDnnhNvz77NysS+/BKUVVQCAiqrqgL1zvt16BD9k5UYtRoquQPkyknSqx9q3B45zqUUralAJHQCypo7Be3ecZ2gMV73xk9vzffklGPm3pbjvQ8f8Lxe9/AN6P/mN1/duPXQSkz7ZiN+/vw53zl7r9zz7j5VwRkiLC3Xi0vmbD2H0a8uwKPNwlCIiM2twCT0xPrZe5nkJx3dajftIYalbL4OTpytqHo+btgLz1h8M6ngXv7IUw//KSYusrOBUReCdXOzQFmLZebQ4GuGQyTW4hG5m3rqwfZJxAP2fXVzzRQ1VcZn3mSLJJKLUiK5HE/iT/81EWWVV5AeiesOEbiJKAbN/2ufyXGHpDketfefRopAvv4ki8f7q/fhq46Gw3rsnrxiZB0/qHBEFwoRuMs98va3m8a+nr8LCLY7JvkR8D/Y4crLU7YfAm8LSCuzS4WYZWYNeKx6FW4UY9eqPuPL1FfoEQUELZgm6WSKSKyJeZ7wXkVtFZLP230oR6a9/mPr736SLse6JS3FRz2SjQ/EpY39BzeNvthzxOaXp3XPW4pmvt+HgCd89E27+12pc9toy3WOkcHlPlSVllcjOL6nnWHwza7dK8i6YGvpsAGP8vL4PwMVKqXMATAUwQ4e4oq5H22Zo06wR3r9riNGhBGXBlsMYN61ujee/Gw7ihHbjzN/ovm3adK378ksw9p/LcTLEm20UHeKRMu98by1G/G2pMcE0IHZtvQyY0JVSywAc9/P6SqWUsyq5GkA9D8dsOHbn1u258ODHG0P6cL7+3S5sO1yIJduP+twnt7AU767YZ7o2+8/X5SCnwF7z3hSVuv+wrsl2fNUqIxx6b7L/dVRP9G5DvwuA9w7UAERkgohkiEhGXl6ezqeO3IU9zLkcWKTWuTTdOPm7lP7Dh+sxdf427DXg0v/NH3bj74t31NleWVWNhz7dhGEv26MbpjPhjvnncq+vcy6V6NLrHoPZ6JbQRWQkHAn9z772UUrNUEqlK6XSU1Kiu/hEOKps+iW6/u2VNY/nbajtw742+ziOnCyts7+z37sRf49Xvt2Bad/vrrPdnv9nELX5+u2asMg/XRK6iJwDYCaAq5VSx/Q4phGsns9D/RL/evoqXPbaj27b1v9SgF1emnaMZtYmhJ9252PGssATsHlyTvPgi1nLS+YWcUIXkS4A5gH4rVJqZ+Qh1b9rB3YCYN3LXGfvFgkhoy/a6ugOWVTqPvDourdWetvdcMqkdfRbZ/6MFxdmhfSeyqpqlJRbY8BOKJ8pMl4w3RbnAlgFIE1EckTkLhGZKCITtV2eAtAGwFsislFEMqIYb1S8dtMAZL80Do9cnmZ0KBF5ceF2rM0+DqUUBj632O++S7bVvSl6rLjuknpmYaca6+kAtXMg8h8wO/29KHgB12dTSo0P8PrdAO7WLSIDDenWBvdc1BXvLPc/SMesFmw+jAWbD+PZq/qGPAcIAFzwkvvyePlFZejVLkmv8EgTzIXgb99dE/1AgjBvfQ5uOJcd16yCI0V9mHxFb6NDCNuu3PBGhJZ53KC7ZebP2B3ksU6XVyG3qO4N1nCd9miSsFWNM4iy/BLhsoR6tZSs3HMMK3fn63MwE7HV58kFE7oPzu/DQ5f1MjSOcOj5Yc3ODy6x3PivVRj8wne45G9L6/StDsfts9xrqF9vDm9OETMqKnP/+5i9b/2qvdbo58CJxJjQ62jayNEK1TghFtkvjcP9o3oaHFHoQl0+7MQp33OmB7tyzRZtIqa9+SVY/8uJAHsH5hxg4/ToZ5sjPmYwlFKoqIruivd/XeTez97sk1hFUkGYOn8btuTUlm/roZOojNLfd/LnW9ye7z9Wgkkfb/T6/9Ou93qZ0D1MvLg7Hrk8DeMHd/G5zwP1vJRdqD4Pcr50pxv/tcrna0oprN57LGr9pc3mzR92o+eUb1Cow1WGL6c8mpPiYvT/Gup5lRbJDdp3V+yr+XztPFqEcdNW4G+Lo9MZbtlOx2BF55TRj3y2GfM2HPQ6sM7pw59/iUosRmFC95AYH4v7RvZAfKzvP82k0WlYO+XSeowqNKEmX3+LIWw/XISbZ6zG1PnbfO7jSa8pA4zoRvpJRg4A4Hix+1XL6fIqXZZ2c3AvV1yso7qYOnmB171nLg9ubdpo0evHwTmILVpXJCKCrzcdQr+nv8XWQ8Gd45/f7TL9FVIomNCDdFaH5m7PWzdNMCiS6PCVhEu02s7mED70M3XqJfTeymxdjuPNrqNFIV2KP/DRBox+bRlOlUe+YIjnn9pf5QEAnl+wHev2F3jtauqLnk0Kev2sOpsCo9XcESOOrrsAsO1QYc32QD9Inp0BrIwJPQh7XxyLBfcPc9sWGyN47SZLzBQclLeWeh/tOHOFlpxDqKat0HpFnC6vQkEEa5oe8NHTI9Aoy2COe9lry2q+/MFYveeYdm7vX35/9yE8ef4ll2w7GvBq5Pq3V+KeOcYM8Xjbx2cjWM4E7ixhTJgZXSmFvy/x3VwTI4LD2lVAjNTOY/ns11vr9JryOHJY8ZgRE3oQYmIEMTGC7ilN3bZ3bNHYoIj098q3dSfEitS415dj4NQlus8Jc9jL/DOhcC6c7dq2eu8H65D+/BKf73GWYNDUJVizr+7kowOeq31vaUUV5q3P8XnV47l99spsvP1jZEkzUsF2Tw3HqfIqTPp4Y01TYEyYNfSjhWWY9t0un6+7HrfgVHnN/7OsI0WYHcWrPTNhQg/BvHsvxLcPDq95bp/f9cBCvSz9ZO0B7M1zzNbY/fGFSJ28IOQ28Wql8PKirDrd+h78aENIx/Flc85JPPrZJgDAN5lHkO/Sbu4ZqWsSnr3Sf5PS8wu2YdInm3x29/P2V9C7HTfUdu99AbqnRtoXfd6Gg9iqNYPEhpHRb3lnNYb+5Tu/+7hOU/D8gu1uFQnPnl/sh05o0SQeae1rR04mJQYcaGsbWUeKsP1wIZ760uvCVXU8+nndboaZQd6ocpqzaj/eXrqnzpS5m3LCT37/XpmNDb/U1sydN0GdfKUa1+//wi1HcO7UJV6bk06XV+GD1Y6eE57z5Dh5+13LN/G0C4BjkFmkVuxy9EIJZ36YlXsC94X3/KFwvQLz1XwHAHlF4TcLmg0TegT6dmyB9+8abHQY9eaKfy7HnFX7sf1wIY6XlIfclr3Py/zqr3+3C+8sC70XR6C2+cmfb/baa+Tpr7a6rdsaLM8a3bGScry1tO40v8t31c7z76sW6K0pZm227651roLtaWNEP+uqauV3UJlzfEK4TS6B+Cuz58hb1x/QiR+si05ABmBCj9BFPc03r3u07T9WgkFTl6D3k4uQOnlB0CP0Plxd2+d3x5EipE5egFeX7MQLIdycdBo41Xd7NwB8tPYAgPBvoHq+z9tgLc+atmeivvdD/RPFL8eCG1UaapOCa+xj+rYP7c2aZ7/eirOfWRzw8xDuTdFA9vv52wQ658DnFkdtwFN9YkLXQYvG8UaHUK9+3OnenvrZuhwfe7pzHf25WJu+NxJz1/yC1MkL/A4CCiaxORf0ABw33gBHDd/tOF7e9+4K97b0ro8txIT3a5N4NNpp756TgYKSct1Hs7qG2rl1eDf752kD2gLdb/GWXKuqoztCNybAZUHBqQpM/3EPUicvcOvyaDVM6DrY9PTomsdTr+6L284/E+PO7mBgRNE1d4376LopXwTXru7KM9flFpXima+2hnSMx+Y5hnpf+NL3+ON/1uOFBduQU3DKrb3UdVDO9sPev6iFLgndObWtazt9blGpriNlI030A6cuwYMfbdQnGI3zbwkAfxjhfST08QDNXM4RmoHq394qy7fPWoOeU7yvXqlHd81gmnm+3nQYAPBN5uGIz2eUhnNXL8p6t0/CsB7J+O35qQAcl+xHCkv9DjtuiCqqqhEfG1MnqQ1+wX8PBn+KSisxf7PjS+g59fGrS3bi/lE9caq8Elf4WL/zk4wDfo9/mw5T2ZZWVOGxeVvw2NjeIc+1482CLYfxZsRHqeWarFv5GDR334frMXfC0DrbC0rKQ2o2m7/5MDq02IYp4/rgVHklYmOkZuzC7J/24Y4LuwIA8orKHAuahzCgyhfnVcHJ0xVIjPdej92h3Z+w8qIeTOg6WeTSnRFwTCHQpXUTrNtfgKYJsVg5eRT6eyw60a9Tc2QetO7lXThunfmz137c0XTwxGk0jo/1+frrXtYwdZV1JPw+2vvyS7DraBEe/2IL8ovLESMSlaaY4rJKbDpQOymaMyfNW5+Dzq2b4LzU1j7f62vKAU9ZRwqx9dBJpCQ1QtukRACOaSY872cE85l+Z/k+tGqaUGeisme+3oaBXVqhcUIsRr+2zO8xFjwwDOOmrQgq9syDJ7HtUCHGTluOod18/y2AwFcYZhYwoYvILABXAshVSvXz8npvAO8BGARgilLqb7pHaXHPXd0PLZrUbWeff/9FGD9jtWWmJ9VDfSdzwNEk069T88A7esjOL8GJ05FN0jXyb0vdnn++Prj7DaH608cbsWTbUdyY7liMQing+6yjmPSJo5999kvjIj5HwamKmgTqPN7r39cd6DP+ndVBHc8zmTtd/eZPSAtiYZW+HVsEdR4AyC0qw9hpjiu01Xv9fwZdK+jbDxeiUVwMissqseXgSdw65Mygz2mEYGroswG8AWCOj9ePA3gAwDU6xWQbzs+Fa4VsZFoKnvpVX2Rp7blRmGiPvAjnSmiERzI2q715xTXNEsdLan+Afje7tu25sLQCp8qqkBgfg5LyKnRq6f/G56Nj0nwmXMAxZ0pifCyORTC1gz87dJsILXT/+N8u5BaV4aHLetVpprt2YCccOnEal/59Gd65LR1784ox6qx26NG2GaqqFZRSiAswN080STAz44lIKoD53mroLvs8A6A42Bp6enq6ysiw3PKjIcktLMULC7fjpevOQeOEWOQWlaJF43g0iqu9/L9nToYubYREesl+aRxKK6rQ+8lFRofiV/ZL44JuLjKTOy9MxdO/6hv2+0VknVIq3dtr9fpTIiITRCRDRDLy8vICv8Hi2jZPxD9vHojGCY4E3jYp0S2ZA8CvuV4jmVCin3sOFJn3fsqO2rHrNaErpWYopdKVUukpKQ1vQI43o/u2r5PU/zfpYr/v+dOlvdCzbTPccUFqFCMjIqthC64JdHRpz7wx/Qz0aNus5nnW1DFe37Nk0sV45qrwL9uI7CBa0whYFbstmsAfL+mBxPhYvLwoC9cNctTW10wZBYF4vfS1cDdZIl11bt3E75D/hiZgDV1E5gJYBSBNRHJE5C4RmSgiE7XX24tIDoBJAJ7Q9gm9j1gDFh8bg3tHdEf2S+MwtFsbAI729pSkRgHfyxqKvd1zUVejQzC1j7wMdLKCcCakC0bAhK6UGq+U6qCUildKnaGUelcpNV0pNV17/Yi2vblSqqX2uGGNlomy+/0sSr31We9NMmQPZ7RqYti5P5t4vmHnDlaHFo0tecX64c/7o3JctqFbwEOj09xunCbE1f5vc/agIXuqNGChbKd0P6NLzeSFa842OoSQRev/KhO6RXRuXVtTSwhi4MLFvSLrRbTizyMjej/pIyHWgtXPenbLkC4454zgR42aQbTa/ZnQLeIPI7rjop7JAICuye5rmy54YFid/ftrH3Dne3z54eERdbaNSEtBnAmHsD48upfRIad1zcEAAAseSURBVNSbf9w0AIsevAg3ndfF6FAsIZxl7ezIfN9a8iouNgb/vnMwPr/3Aozs3dbtNdc5LW5K7wzA0RXyzVsG4fXxA/GX687Gn8f0xvJH3Wvdb986CO2bJ9Y51+w79VuF6f27BmP2nefpcqwJw7vrcpxIxMYIurSOfrv2NQM7oXf75m7Na0ZYOfkSQ88frHsvNv6zYQZM6BYSEyM498xWfvdx3iCqVsC4czqgZZMEjB/cBfeO6I7OrZvUzNN+/yU9MKZfezROiEX2S+Pw/l2DMf/+YVg75VIAQPPGtT1a/Q1g8jbp1du3DgLgaOu/qGdK2Ks6bXpqND6/9wIAQM+2zQxPbgBw97Cuuvcsch134M0fR/q+Ka43zxuhHQPM+WKU/xvV0+1500a1n9drB3aq73BMg/3QbWLpwyOQW1SGBZsPAQB8NbO/eesgr/NoeybdJglx2PPiWMQIUFhaiZ9252NXbjFixH3ptc8mXlBnzo8rzu6AOb8bXNM0FBsjSIiNQXmQK9I0T4xDobbA8qAuLfHitWfjyv76LRhyQfc22JNXXLM6USgeG3sWvsl0rLaUEBdTs/DFX68/p87C2PPvH4YrXw9uetcNT14GwPGD7Pl3evjyNDx8eVq9zFvi7UborDvS3Sb6CtbQbq3x4d1Dayas0iv+WXek45Le7dy2tW/huNJ8YFRP3H1RV+QWleKn3ceQlBiHRnExyC/2PonYu7en465/22dOKeOrPKSL1OSmGNy1NR66PA0ThnfDtQMjnyMmNkYgImjROB5/v3EAAKB3+9oa+eDU1kiMj0W6dtVw3cBOmP6bcwEAw3uluN3I7dOxbk3+0rPa1dkGAO/clo7RfdohKTEOIoJbhnRB80TH9MO3DqltU27jsRDDnRemYt0TlwYsV1r7JCQ3C9zH35fpvzkX1w86A0+MO6tm243ndcbWZy93269fJ/836tomNUKvds3w3FV90appAlo1TUDLJgk1c42bxSW92+G6MGq9tww5E7ExUmf2wQUPDMPM2+rOLTWwS8ugjtuzbd2pdbunNMMPD4/A/43qieaJ8fi91jw3oLP/Y0ZjbvpgBNOxIRxM6DbTPDEej489S/fmidRkR3K+/twz8JfrzsasO9LxiXZ5/tpNA/DboWfilV/3x5h+3hcY/vedgzH3nqEeX1rldd7rId3aYMZt6V7XgXzh2touaov/NBxNEmLxhxGOL+8F3ZPRJohEHSOCd28PvV3/Fu3HpE/H5nj1xv410yP/Zqhju+tl/93DHAOC5t4zFI+P7V2zPbWN4++Y1CgOa6ZcisV/uhgX9PB/49qpd/vAc4RHyw0hTCLXv3NLrHviUlzVv6Pb9vfuPA/LHx2Jvh1bYERaCsad437V9cjlaX6P2zQhFjueH+NWUXDVNblpzc3RQWe2QtfkpnhodBou6+N70etBZ7ZCjAAtm8TjAY9mnGi6tE/bwDuFIajpc6OhIUyfazdKqYiX56quVuj2+EIAwKTLeuHm8zpjbXYB2rdohOvfXgUg8GIM+cVlKKusdpvTO7eotKZm++hnmzC4axt8sHo/Nrqs4vPObem4Z04GVvx5JM5o1QSZB0/iytdXoGOLRIw6qx3eX+0Y7HFJ77Z45ld9MfyVH9zO6xlXblEprnnjJ7x/9xB0T3G0g3+79QgOFpzG74bVjvA8caocA55bgpSkRlg75VKUV1ZDxDFCOBRTvtiCD3/+Bb+/uBv+u+Gg1yajjyYMxc0zgltgwtNjV/TG7/3cXDxVXok+T31b83xI19ZIbtYIC7Y4lv+bMvYsnDhdjocuSwu4KDMAtyl6r+rfEdPGDwTgWA5vxe58PDB3g9v+j4/tHdaN8YqqahSUlGPWT9mY/uOemu2PXJ6G+0b2QFllFQRSUwnq8fjCqPf/v6Jfe7ytXc2Gyt/0uUzoZIisI4Xo1TbJ7Yt/6MRplFVW1+mWGaniskpsP1zodRm2yqpqiIjPbm9fbjyInm2T0C2lqeFTypZXVmPn0SL069QCFVXVeOmbLKQkNUL6ma2wNrsAw3slo2/HFjhWXIYqpfC/bbm4oHsbnNmmCZbuyENcrOCN73cjY38BqqoVuqc0Rf/OLXHg+CnM+d2QoAeprdtfgLT2SWimXZHkFpUit7AsYBOTNwUl5UhKjPO7KERRaQUqq5TPtU6DVV5ZjRW78+q0v3uz/pcCbMk5ifNSW2NXbhEGd22NtkmJeOTTTVAAbr8gFWd1SEKjuFicLq/CpE82YuWeYzgZ5ApX/7hpAK4J8+YtEzoRkU2YZoELIiKKHiZ0IiKbYEInIrIJJnQiIptgQicisgkmdCIim2BCJyKyCSZ0IiKbMGxgkYjkAQh3Yb1kAPk6hmMklsV87FIOwD5lsUs5gMjLcqZSyuuc1IYl9EiISIavkVJWw7KYj13KAdinLHYpBxDdsrDJhYjIJpjQiYhswqoJfYbRAeiIZTEfu5QDsE9Z7FIOIIplsWQbOhER1WXVGjoREXlgQicisgnLJXQRGSMiO0Rkt4hMNjoeTyIyS0RyRSTTZVtrEVkiIru0f1u5vPaYVpYdInK5y/ZzRWSL9to0iXTtt/DK0llEfhCR7SKyVUT+z4rlEZFEEVkjIpu0cjxrxXK4xBArIhtEZL7Fy5GtxbBRRDIsXpaWIvKZiGRp35fzDSmLUsoy/wGIBbAHQDcACQA2AehjdFweMQ4HMAhApsu2vwKYrD2eDOBl7XEfrQyNAHTVyharvbYGwPkABMA3AK4woCwdAAzSHicB2KnFbKnyaOdspj2OB/AzgKFWK4dLeSYB+A+A+Rb/fGUDSPbYZtWy/BvA3drjBAAtjShLvRZahz/a+QC+dXn+GIDHjI7LS5ypcE/oOwB00B53ALDDW/wAvtXK2AFAlsv28QD+ZYJyfQngMiuXB0ATAOsBDLFiOQCcAeA7AJegNqFbrhzaebNRN6FbriwAmgPYB62TiZFlsVqTSycAB1ye52jbzK6dUuowAGj/ttW2+ypPJ+2x53bDiEgqgIFw1G4tVx6tmWIjgFwAS5RSliwHgH8AeBRAtcs2K5YDABSAxSKyTkQmaNusWJZuAPIAvKc1hc0UkaYwoCxWS+je2pOs3O/SV3lMVU4RaQbgcwAPKqUK/e3qZZspyqOUqlJKDYCjhjtYRPr52d2U5RCRKwHkKqXWBfsWL9sML4eLC5VSgwBcAeA+ERnuZ18zlyUOjmbWt5VSAwGUwNHE4kvUymK1hJ4DoLPL8zMAHDIollAcFZEOAKD9m6tt91WeHO2x5/Z6JyLxcCTzD5VS87TNli2PUuoEgKUAxsB65bgQwFUikg3gIwCXiMgHsF45AABKqUPav7kAvgAwGNYsSw6AHO2qDwA+gyPB13tZrJbQ1wLoKSJdRSQBwM0AvjI4pmB8BeB27fHtcLRFO7ffLCKNRKQrgJ4A1miXZ0UiMlS7y32by3vqjXbudwFsV0r93eUlS5VHRFJEpKX2uDGASwFkWa0cSqnHlFJnKKVS4fjsf6+U+o3VygEAItJURJKcjwGMBpAJC5ZFKXUEwAERSdM2jQKwDUaUpb5vhOhwA2IsHL0t9gCYYnQ8XuKbC+AwgAo4fnHvAtAGjhtZu7R/W7vsP0Uryw643NEGkA7HB3wPgDfgccOlnsoyDI5Lvs0ANmr/jbVaeQCcA2CDVo5MAE9p2y1VDo8yjUDtTVHLlQOOdudN2n9bnd9lK5ZFi2EAgAztM/ZfAK2MKAuH/hMR2YTVmlyIiMgHJnQiIptgQicisgkmdCIim2BCJyKyCSZ0IiKbYEInIrKJ/wdehog/81SqrAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossrec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12916666666666665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f891e387150>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gresults = [1 - np.sum(np.diag(cm))/np.sum(cm) for cm in results[0]]\n",
    "print(min(gresults))\n",
    "plt.plot(gresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need uneven kernels to go full klein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got 93.3% accuracy\n",
    "# class lstm_Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.convRescale = nn.Conv3d(1, 180, (5,7,7), bias=False)\n",
    "#         self.conv1 = nn.Conv3d(180, 100, (1,8,5))\n",
    "#         self.conv2 = nn.Conv3d(100, 100, (1,8,5))\n",
    "        \n",
    "#         self.batchConv1 = nn.BatchNorm3d(180)\n",
    "#         self.batchConv2 = nn.BatchNorm3d(100)\n",
    "#         self.batchConv3 = nn.BatchNorm3d(100)\n",
    "        \n",
    "#         self.drop1=nn.Dropout3d(.2)\n",
    "                \n",
    "#         x= torch.randn(1,1,*imageSize)\n",
    "#         self._to_linear = None\n",
    "#         self.convs(x)\n",
    "        \n",
    "# #         self.fc1 = nn.Linear(self._to_linear, 200)\n",
    "#         self.batch1 = nn.BatchNorm1d(200)        \n",
    "#         self.lstm = nn.LSTM(self._to_linear,200)\n",
    "#         self.fc2 = nn.Linear(200, 6)\n",
    "\n",
    "        \n",
    "#     def convs(self, x):\n",
    "#         x = F.relu(F.max_pool3d(self.convRescale(x),(1,3,3)))\n",
    "#         x = self.batchConv1(x)\n",
    "# #         x = self.drop1(x)\n",
    "#         x = F.relu(F.max_pool3d(self.conv1(x),(1,2,2)))\n",
    "#         x = self.batchConv2(x)\n",
    "#         x = F.relu(F.max_pool3d(self.conv2(x),(1,2,2)))\n",
    "#         x = self.batchConv3(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "#         if self._to_linear is None:\n",
    "#             self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]*x[0].shape[3]\n",
    "            \n",
    "#         return x\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         hidden = [torch.randn(1, x.shape[0], self._to_linear).to(device),torch.randn(1, x.shape[0], 200).to(device)]\n",
    "#         out = torch.zeros(12,x.shape[0],200).to(device)\n",
    "#         for i in range(x.shape[1]):\n",
    "#             xi = x[:,i,...].view(x.shape[0],1,*imageSize)\n",
    "#             xi = self.convs(xi)\n",
    "#             print(xi.shape)\n",
    "#             xi = xi.view(-1, self._to_linear)\n",
    "#             out[i,...] = xi\n",
    "#         del xi\n",
    "#         del x\n",
    "#         out, hidden = self.lstm(out,hidden)\n",
    "#         x = F.relu(out[-1])\n",
    "#         x = self.batch1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters77 = np.load('/home/ephy/Projects/tda_convolve_video/src/python3/VideoFeatures_555_movAndRot80.npy', allow_pickle=True)\n",
    "class lstm_Net(nn.Module):    \n",
    "    def __init__(self):        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstmOutSize = 50\n",
    "        \n",
    "        self.convRescale = nn.Conv3d(1, 20, (5,5,5), bias=False)\n",
    "        self.conv1 = nn.Conv3d(20, 40, (1,5,5))\n",
    "        self.conv2 = nn.Conv3d(40, 320, (1,8,5))\n",
    "        \n",
    "        self.batchConv1 = nn.BatchNorm3d(20)\n",
    "        self.batchConv2 = nn.BatchNorm3d(40)\n",
    "        self.batchConv3 = nn.BatchNorm3d(320)\n",
    "        \n",
    "#         self.drop1=nn.Dropout3d(.5)\n",
    "                \n",
    "        x= torch.randn(1,1,*imageSize)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        #         self.fc1 = nn.Linear(self._to_linear, 200)\n",
    "        self.batch1 = nn.BatchNorm1d(self.lstmOutSize)     \n",
    "        self.batch2 = nn.BatchNorm1d(6)\n",
    "        self.lstm = nn.LSTM(self._to_linear,self.lstmOutSize)\n",
    "        self.fc2 = nn.Linear(self.lstmOutSize, 6)\n",
    "        \n",
    "    def convs(self, x):\n",
    "        x = F.relu(F.max_pool3d(self.convRescale(x),(1,3,3)))\n",
    "        x = self.batchConv1(x)\n",
    "#         x = self.drop1(x)\n",
    "        x = F.relu(F.max_pool3d(self.conv1(x),(1,2,2)))\n",
    "        x = self.batchConv2(x)\n",
    "        x = F.relu(F.max_pool3d(self.conv2(x),(1,2,2)))\n",
    "        x = self.batchConv3(x)\n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x.shape[1]*x.shape[3]*x.shape[4]\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        batchSize = x.shape[0]\n",
    "        x = torch.transpose(x,0,2)\n",
    "        x = torch.transpose(x,1,2)\n",
    "        x = x.reshape(x.shape[0],x.shape[1],np.prod(x.shape[2:]))\n",
    "        hidden = [torch.randn(1, batchSize, self.lstmOutSize).to(device),torch.randn(1, batchSize, self.lstmOutSize).to(device)]\n",
    "        x, hidden = self.lstm(x,hidden)\n",
    "        x = F.relu(x[-1])\n",
    "        x = self.batch1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch2(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = lstm_Net().to(device)\n",
    "torch.cuda.memory_allocated()*1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "# Batch size of 26+1, because normalization requires same batch size and 4472%26=0 (very close to even split) (same logic for training)\n",
    "testingBatchSize=100\n",
    "trainingBatchSize=100\n",
    "results = [[],[],[]]\n",
    "lossrec = [[],[],[]]\n",
    "lr=2e-3\n",
    "\n",
    "for foldIndex in range(1):\n",
    "\n",
    "    net = lstm_Net().to(device)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    # Set filters in 2 layers\n",
    "    with torch.no_grad():\n",
    "        for i,weights in enumerate(filters77):\n",
    "            net.convRescale.weight[i] = torch.nn.Parameter(torch.tensor(weights)).to(device)\n",
    "        net.convRescale.requires_grad=False\n",
    "\n",
    "    print('Network Reset.')  \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_image, labels,\n",
    "                                                        shuffle=True, random_state=foldIndex,\n",
    "                                                        test_size=0.1, stratify = stratLabels)\n",
    "\n",
    "    y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "    X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "    X_test =  torch.tensor(X_test,dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "\n",
    "    trainingSeq = list(range(0,y_train.shape[0],trainingBatchSize))\n",
    "    trainingSeq.append(y_train.shape[0]+1)\n",
    "    trainingSeq = np.array(trainingSeq)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "#         if epoch!=0 and epoch%15==0:\n",
    "#             lr = lr*1e-10\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = lr\n",
    "        for i in range(len(trainingSeq)-1):\n",
    "            X=X_train[trainingSeq[i]:trainingSeq[i+1]].view(-1,1,*imageSize)\n",
    "            X=X.to(device)\n",
    "            y=y_train[trainingSeq[i]:trainingSeq[i+1]]\n",
    "            y=y.to(device)\n",
    "            net.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = nn.CrossEntropyLoss()(output, torch.argmax(y, axis=1))\n",
    "            lossrec[foldIndex].append(float(loss.tolist()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del loss\n",
    "            del output\n",
    "            del X\n",
    "            del y\n",
    "            \n",
    "        print('Fold: ', foldIndex, 'Epoch: ',epoch)\n",
    "        results[foldIndex].append(test(net, X_test, y_test, testingBatchSize))\n",
    "        cm = results[foldIndex][len(results[foldIndex])-1]\n",
    "        print('Accuracy: ' , np.sum(np.diag(cm))/np.sum(cm))\n",
    "\n",
    "# np.save('/home/ephy/Projects/tda_convolve_video/data/altered/KTH_net8_k1layer_30_res.npy', results)\n",
    "# np.save('/home/ephy/Projects/tda_convolve_video/data/altered/KTH_net8_k1layer_30_loss.npy', lossrec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gresults = [np.sum(np.diag(cm))/np.sum(cm) for cm in results[0]]\n",
    "max(gresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(lossrec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[1,2,3],[3,4,3]],[[9,9,9],[9,9,9]]]*5)\n",
    "print(a)\n",
    "# print(a.shape)\n",
    "a = a.reshape(a.shape[1],a.shape[0]*a.shape[2])\n",
    "# print(a.shape)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
